#!/usr/bin/env python3
"""
Code to run simulations, inference methods and generate all plots
in the paper.

Run as e.g. 

./plots.py setup metrics_by_mutation_rate -P
./plots.py infer metrics_by_mutation_rate -P -p 30 -t 8 #this may take a long time
./plots.py figure kc_rooted_by_mutation_rate

"""

import argparse
import collections
import filecmp
import glob
import itertools
import json
import logging
import multiprocessing
import os.path
import sys
import random
import shutil
import signal
import statistics
import subprocess
import tempfile
import time

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as pyplot
import pandas as pd
import tqdm

# import the local copy of msprime in preference to the global one
curr_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(1,os.path.join(curr_dir,'..','msprime'))
import msprime
import msprime_extras
import msprime_fastARG
import msprime_ARGweaver
import msprime_RentPlus
import ARG_metrics

fastARG_executable = os.path.join(curr_dir,'..','fastARG','fastARG')
ARGweaver_executable = os.path.join(curr_dir,'..','argweaver','bin','arg-sample')
smc2arg_executable = os.path.join(curr_dir,'..','argweaver','bin','smc2arg')
RentPlus_executable = os.path.join(curr_dir,'..','RentPlus','RentPlus.jar')
tsinfer_executable = os.path.join(curr_dir,'run_tsinfer.py')
ftprime_singlelocus_executable = os.path.join(curr_dir,'..','ftprime_ms','sims','run-singlelocus.py')
ftprime_multilocus_executable = os.path.join(curr_dir,'..','ftprime_ms','sims','run-sim.py')

#monkey-patch write.nexus into msprime
msprime.TreeSequence.write_nexus_trees = msprime_extras.write_nexus_trees

FASTARG = "fastARG"
ARGWEAVER = "ARGweaver"
RENTPLUS = "RentPlus"
TSINFER = "tsinfer"

#R tree metrics assume tips are numbered from 1 not 0
tree_tip_labels_start_at_0 = False

if sys.version_info[0] < 3:
    raise Exception("Python 3 only")

save_stats = dict(
    cpu = "cputime",
    mem =  "memory",
    n_edges = "edges",
    ts_filesize = "ts_filesize"
)

def nanblank(val):
    """hack around a horrible pandas syntax, which puts nan instead of blank strings"""
    return "" if pd.isnull(val) else val

def always_true(*pargs):
    """
    A func that returns True for any input value
    """
    return True

def make_errors(v, p):
    """
    For each sample an error occurs with probability p. Errors are generated by
    sampling values from the stationary distribution, that is, if we have an
    allele frequency of f, a 1 is emitted with probability f and a
    0 with probability 1 - f. Thus, there is a possibility that an 'error'
    will in fact result in the same value.
    """
    w = np.copy(v)
    if p > 0:
        m = v.shape[0]
        frequency = np.sum(v) / m
        # Randomly choose samples with probability p
        samples = np.where(np.random.random(m) < p)[0]
        # Generate observations from the stationary distribution.
        errors = (np.random.random(samples.shape[0]) < frequency).astype(int)
        w[samples] = errors
    return w

def generate_samples(ts, error_p):
    """
    Returns samples with a bits flipped with a specified probability.

    Rejects any variants that result in a fixed column.
    """
    S = np.zeros((ts.sample_size, ts.num_mutations), dtype="u1")
    if error_p == 0:
        for variant in ts.variants():
            S[:,variant.index] = variant.genotypes
    else:
        for variant in ts.variants():
            done = False
            # Reject any columns that have no 1s or no zeros
            while not done:
                S[:,variant.index] = make_errors(variant.genotypes, error_p)
                s = np.sum(S[:, variant.index])
                done = 0 < s < ts.sample_size
    return S

def msprime_name(n, Ne, l, rho, mu, genealogy_seed, mut_seed, directory=None):
    """
    Create a filename for saving an msprime simulation (without extension)
    Other functions add error rates and/or a subsample sizes to the name
    """
    #format mut rate & recomb rate to print non-exponential notation without
    # trailing zeroes 12 dp should be ample for these rates
    rho = "{:.12f}".format(float(rho)).rstrip('0')
    mu = "{:.12f}".format(float(mu)).rstrip('0')
    file = "msprime-n{}_Ne{}_l{}_rho{}_mu{}-gs{}_ms{}".format(int(n), float(Ne), int(l), rho, \
        mu, int(genealogy_seed), int(mut_seed))
    if directory is None:
        return file
    else:
        return os.path.join(directory,file)

def msprime_name_from_row(row, directory=None, error_col=None, subsample_col=None):
    """
    If error_col and subsample_col are None, this is the same as msprime_name()
    but filled out using data from a row. If error_col is a string which exists as
    a column name in the row then add_error_param_to_name() is also called, using
    the error rate specified in that column. Alternatively (e.g. if error_col is a
    number) then it is treated as the error rate to add via add_error_param_to_name().
    The same goes for subsample_col.
    """
    name = msprime_name(row.sample_size, row.Ne, row.length, row.recombination_rate,
        row.mutation_rate, row.seed, row.seed, directory)
    if subsample_col is not None and not pd.isnull(subsample_col):
        if isinstance(subsample_col, str):
            if subsample_col in row:
                name = add_subsample_param_to_name(name, row[subsample_col])
        else:
            name = add_subsample_param_to_name(name, subsample_col)
    if error_col is not None and not pd.isnull(error_col):
        if isinstance(error_col, str):
            if error_col in row:
                name = add_error_param_to_name(name, row[error_col])
        else:
            name = add_error_param_to_name(name, error_col)
    return(name)

def add_subsample_param_to_name(sim_name, subsample_size=None):
    """
    Mark a filename as containing only a subset of the samples of the full sim
    Can be used on msprime output files but also e.g. tsinfer output files
    """
    if subsample_size is not None and not pd.isnull(subsample_size):
        if sim_name.endswith("+") or sim_name.endswith("-"):
            #this is the first param
            return sim_name + "max{}".format(int(subsample_size))
        else:
            return sim_name + "_max{}".format(int(subsample_size))
    else:
        return sim_name
        
def add_error_param_to_name(sim_name, error_rate=None):
    """
    Append the error param to the msprime simulation filename.
    Only relevant for files downstream of the step where sequence error is added
    """
    if error_rate is not None and not pd.isnull(error_rate):
        if sim_name.endswith("+") or sim_name.endswith("-"):
            #this is the first param
            return sim_name + "_err{}".format(float(error_rate))
        else:
            #this is the first param
            return sim_name + "err{}".format(float(error_rate))
    else:
        return sim_name

def construct_fastarg_name(sim_name, seed, directory=None):
    """
    Returns a fastARG inference filename (without file extension),
    based on a simulation name
    """
    d,f = os.path.split(sim_name)
    return os.path.join(d,'+'.join(['fastarg', f, "fs"+str(int(seed))]))

def fastarg_name_from_msprime_row(row, sim_dir):
    """
    return the fa name based on an msprime sim specified by row
    """
    return construct_fastarg_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample_size'),
                                  seed=row.seed)


def construct_argweaver_name(sim_name, burnin, ntimes, seed, iteration_number=None):
    """
    Returns an ARGweaver inference filename (without file extension),
    based on a simulation name. The iteration number (used in .smc and .nex output)
    is usually added by the ARGweaver `arg-sample` program,
    in the format .10, .20, etc. (we append an 'i' too, giving
    'i.10', 'i.100', etc
    """
    d,f = os.path.split(sim_name)
    suffix = "burn"+str(int(burnin)) + "nt"+str(int(ntimes)) + "ws"+str(int(seed))
    if iteration_number is not None:
        suffix += "_i."+ str(int(iteration_number))
    return os.path.join(d,'+'.join(['aweaver', f, suffix]))

def argweaver_names_from_msprime_row(row, sim_dir):
    """
    return multiple argweaver names based on an msprime sim specified by row
    there is one name per argweaver iteration listed in row.ARGweaver_iterations
    """
    return [construct_argweaver_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample_size'),
                                     burnin=row.ARGweaver_burnin, ntimes=ARGweaver_ntimes,
                                     seed=row.seed, iteration_number=it)
                for it in nanblank(row.ARGweaver_iterations).split(",") if it]

def construct_rentplus_name(sim_name):
    """
    Returns an RentPlus inference filename (without file extension),
    based on a simulation name.
    """
    d,f = os.path.split(sim_name)
    return os.path.join(d,'+'.join(['rentpls', f, ""]))

def rentplus_name_from_msprime_row(row, sim_dir):
    """
    return the rentplus name based on an msprime sim specified by row
    """
    return construct_rentplus_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample_size'))

def construct_tsinfer_name(sim_name, subsample_size=None, shared_breakpoints=0, shared_lengths=0):
    """
    Returns an MSprime Li & Stevens inference filename.
    If the file is a subset of the original, this can be added to the
    basename in this function, or later using the
    add_subsample_param_to_name() routine.
    """
    d,f = os.path.split(sim_name)
    suffix = ""
    if shared_breakpoints is not None:
        suffix += "srb"+str(int(shared_breakpoints))
    if shared_lengths is not None:
        suffix += "sl"+str(int(shared_lengths))
    name = os.path.join(d,'+'.join(['tsinfer', f, suffix]))
    if subsample_size is not None and not pd.isnull(subsample_size):
        name = add_subsample_param_to_name(name, subsample_size)
    return name

def tsinfer_name_from_msprime_row(row, sim_dir, subsample_size=None):
    """
    return the tsinfer name based on an msprime sim specified by row
    """
    if subsample_size is None and not pd.isnull(subsample_size):
        return construct_tsinfer_name(
            msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample_size'),
            shared_breakpoints = getattr(self.row,'tsinfer_srb', None),
            shared_lengths = getattr(self.row,'tsinfer_sl', None)
            )
    else:
        return construct_tsinfer_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample_size'),
            shared_breakpoints = getattr(self.row,'tsinfer_srb', None),
            shared_lengths = getattr(self.row,'tsinfer_sl', None),
            subsample_size = subsample_size)


def time_cmd(cmd, stdout=sys.stdout):
    """
    Runs the specified command line (a list suitable for subprocess.call)
    and writes the stdout to the specified file object.
    """
    if sys.platform == 'darwin':
        #on OS X, install gtime using `brew install gnu-time`
        time_cmd = "/usr/local/bin/gtime"
    else:
        time_cmd = "/usr/bin/time"
    full_cmd = [time_cmd, "-f%M %S %U"] + cmd
    with tempfile.TemporaryFile() as stderr:
        exit_status = subprocess.call(full_cmd, stderr=stderr, stdout=stdout)
        stderr.seek(0)
        if exit_status != 0:
            raise ValueError(
                "Error running '{}': status={}:stderr{}".format(
                    " ".join(cmd), exit_status, stderr.read()))

        split = stderr.readlines()[-1].split()
        # From the time man page:
        # M: Maximum resident set size of the process during its lifetime,
        #    in Kilobytes.
        # S: Total number of CPU-seconds used by the system on behalf of
        #    the process (in kernel mode), in seconds.
        # U: Total number of CPU-seconds that the process used directly
        #    (in user mode), in seconds.
        max_memory = int(split[0]) * 1024
        system_time = float(split[1])
        user_time = float(split[2])
    return user_time + system_time, max_memory

def ARGmetric_params_from_row(row):
    """
    Create some ARGmetric params (see ARGmetrics.py) from a row
    We hack the same polytomy seed as inference seed

    This stupidly has to be defined at the top level, since it is
    part of the function passed in to a multiprocessing Pool, and
    hence needs to be 'pickle'able :(
    """
    return {'make_bin_seed':row.seed, 'reps':row.tsinfer_biforce_reps}

class InferenceRunner(object):
    """
    Class responsible for running a single inference tool and returning results for
    the dataframe. Should create results files that are bespoke for each tool, and
    also for each tool, convert these into nexus files that can be used for comparing
    metrics.
    """
    def __init__(
            self, tool, row, simulations_dir, num_threads, compute_tree_metrics=False):
        self.tool = tool
        self.row = row
        self.num_threads = num_threads
        self.compute_tree_metrics = compute_tree_metrics
        #base_fn is used for haplotype matrices etc from the simulation, so must include error, 
        #but not subsample size (as subsampling happens *after* inference, when comparing trees)
        self.base_fn = msprime_name_from_row(
            row, simulations_dir, 'error_rate')
        #source nexus must be the subsampled version, if present
        self.source_nexus_file = msprime_name_from_row(
            row, simulations_dir, subsample_col="subsample_size") + ".nex"
        # This should be set by the run_inference methods.
        self.inferred_nexus_files = None

    def run(self):
        logging.debug("parameters = {}".format(self.row.to_dict()))
        if self.tool == TSINFER:
            ret = self.__run_tsinfer()
        elif self.tool == FASTARG:
            ret = self.__run_fastARG()
        elif self.tool == ARGWEAVER:
            ret = self.__run_ARGweaver()
        elif self.tool == RENTPLUS:
            ret = self.__run_RentPlus()
        else:
            raise ValueError("unknown tool {}".format(self.tool))
        if self.compute_tree_metrics:
            #NB Jerome thinks it may be clearer to have get_metrics() return a single set of metrics
            #rather than an average over multiple inferred_nexus_files, and do the averaging in python
            assert self.inferred_nexus_files is not None
            metrics = ARG_metrics.get_metrics(self.source_nexus_file, self.inferred_nexus_files)
            ret.update(metrics)
        logging.debug("returning infer results for {} row {} = {}".format(
            self.tool, int(self.row[0]), ret))
        return ret

    def __run_tsinfer(self, default_shared_recombinations=False, default_shared_lengths=False, default_subsample=None):
        shared_recombinations = bool(getattr(self.row,'tsinfer_srb', default_shared_recombinations))
        shared_lengths = bool(getattr(self.row,'tsinfer_sl', default_shared_lengths))
        subsample_size = getattr(self.row,'subsample_size', default_subsample)
        samples_fn = self.base_fn + ".npy"
        positions_fn = self.base_fn + ".pos.npy"
        time = memory = fs = counts = None
        logging.debug("reading: variant matrix {} & positions {} for msprime inference".format(
            samples_fn, positions_fn))
        scaled_recombination_rate = 4 * self.row.recombination_rate * self.row.Ne
        inferred_ts, time, memory = self.run_tsinfer(
            samples_fn, positions_fn, self.row.length, scaled_recombination_rate,
            self.row.error_rate, shared_recombinations, shared_lengths, self.num_threads)
        if inferred_ts:
            out_fn = construct_tsinfer_name(self.base_fn)
            if subsample_size is not None:
                out_fn = add_subsample_param_to_name(out_fn, subsample_size)
                inferred_ts = inferred_ts.simplify(list(range(subsample_size)))
            inferred_ts.dump(out_fn + ".inferred.hdf5")
            fs = os.path.getsize(out_fn + ".inferred.hdf5")
            if self.compute_tree_metrics:
                self.inferred_nexus_files = [out_fn + ".nex"]
                with open(self.inferred_nexus_files[0], "w+") as out:
                    #tree metrics assume tips are numbered from 1 not 0
                    inferred_ts.write_nexus_trees(
                        out, tree_labels_between_variants=True,
                        zero_based_tip_numbers=tree_tip_labels_start_at_0)
            unique, counts = np.unique(np.array([e.parent for e in inferred_ts.edges()], dtype="u8"), return_counts=True)
        return  {
            save_stats['cpu']: time,
            save_stats['mem']: memory,
            save_stats['n_edges']: None if counts is None else np.sum(counts),
            save_stats['ts_filesize']: fs,
            'mean_polytomy': None if counts is None else np.mean(counts),
            'var_polytomy': None if counts is None else np.var(counts),
            'max_polytomy': None if counts is None else np.max(counts)
        }

    def __run_fastARG(self):
        inference_seed = self.row.seed  # TODO do we need to specify this separately?
        infile = self.base_fn + ".hap"
        out_fn = construct_fastarg_name(self.base_fn, inference_seed)
        time = memory = None
        logging.debug("reading: {} for fastARG inference".format(infile))
        inferred_ts, time, memory = self.run_fastarg(infile, self.row.length, inference_seed)
        edges = inferred_ts.num_edges
        inferred_ts.dump(out_fn + ".hdf5")
        fs = os.path.getsize(out_fn + ".hdf5")
        if self.compute_tree_metrics:
            self.inferred_nexus_files = [out_fn + ".nex"]
            with open(self.inferred_nexus_files[0], "w+") as out:
                inferred_ts.write_nexus_trees(
                    out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
        return {
            save_stats['cpu']: time,
            save_stats['mem']: memory,
            save_stats['n_edges']: edges,
            save_stats['ts_filesize']: fs,
        }

    def __run_RentPlus(self):
        infile = self.base_fn + ".dat"
        time = memory = None
        logging.debug("reading: {} for RentPlus inference".format(infile))
        treefile, num_tips, time, memory = self.run_rentplus(infile, self.row.length)
        if self.compute_tree_metrics:
            self.inferred_nexus_files = [construct_rentplus_name(self.base_fn) + ".nex"]
            for fn in self.inferred_nexus_files:
                with open(fn, "w+") as out:
                    msprime_RentPlus.RentPlus_trees_to_nexus(
                        treefile, out, self.row.length, num_tips,
                        zero_based_tip_numbers=tree_tip_labels_start_at_0)
        return {
            save_stats['cpu']: time,
            save_stats['mem']: memory,
        }


    def __run_ARGweaver(
            self, n_out_samples=10, sample_step=20, 
            default_ARGweaver_burnin=1000, default_ARGweaver_ntimes=20):
        """
        The two default values are used if there are no columns which specify the 
        number of burn in iterations or number of discretised timesteps
        """
        inference_seed = self.row.seed  # TODO do we need to specify this separately?
        burnin = getattr(self.row,'ARGweaver_burnin', default_ARGweaver_burnin)
        n_timesteps = getattr(self.row,'ARGweaver_ntimes', default_ARGweaver_ntimes)
        infile = self.base_fn + ".sites"
        time = memory = None
        filesizes = []
        edges = []
        iteration_ids = []
        stats_file = None
        self.inferred_nexus_files = []
        logging.debug("reading: {} for ARGweaver inference".format(infile))
        out_fn = construct_argweaver_name(self.base_fn, burnin, n_timesteps, inference_seed)
        iteration_ids, stats_file, time, memory = self.run_argweaver(
            infile, self.row.Ne, self.row.recombination_rate, self.row.mutation_rate,
            out_fn, inference_seed, n_out_samples, sample_step, burnin, n_timesteps,
            verbose = logging.getLogger().isEnabledFor(logging.DEBUG))
        for it in iteration_ids:
            base = construct_argweaver_name(self.base_fn, burnin, n_timesteps, inference_seed, it)
            if self.compute_tree_metrics:
                nexus = base + ".nex"
                with open(nexus, "w+") as out:
                    msprime_ARGweaver.ARGweaver_smc_to_nexus(
                        base+".smc.gz", out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
                self.inferred_nexus_files.append(nexus)
            try:
                #if we want to record number of coalescence records we need
                #to convert the ARGweaver output to msprime, which is buggy
                with open(base+".TSnodes", "w+") as msprime_nodes, \
                        open(base+".TSedges", "w+") as msprime_edges:
                    msprime_ARGweaver.ARGweaver_smc_to_msprime_txts(
                        smc2arg_executable, base, msprime_nodes, msprime_edges)
                    msprime_nodes.seek(0)
                    msprime_edges.seek(0)
                    inferred_ts = msprime.load_text(
                        nodes=msprime_nodes, edges=msprime_edges).simplify()
                    inferred_ts.dump(base + ".hdf5")
                    filesizes.append(os.path.getsize(base + ".hdf5"))
                    edges.append(inferred_ts.num_edges)
            except msprime_ARGweaver.CyclicalARGError as e:
                logging.info("Exception caught converting {}: {}".format(
                    base + ".msp", e))
        results = {
            save_stats['cpu']: time,
            save_stats['mem']: memory,
            save_stats['n_edges']: statistics.mean(edges) if len(edges) else None,
            save_stats['ts_filesize']: statistics.mean(filesizes) if len(filesizes) else None,
            "iterations": ",".join(iteration_ids),
        }
        return results

    @staticmethod
    def run_tsinfer(sample_fn, positions_fn, length, rho, error_probability, 
        shared_recombinations, shared_lengths, num_threads=1):
        try:
            with tempfile.NamedTemporaryFile("w+") as ts_out:
                cmd = [
                    sys.executable, tsinfer_executable, sample_fn, positions_fn,
                    "--length", str(int(length)), "--recombination-rate", str(rho),
                    "--error-probability", str(error_probability),
                    "--threads", str(num_threads), ts_out.name]
                if shared_recombinations:
                    cmd.append("--shared-recombinations")
                if shared_lengths:
                    cmd.append("--shared-lengths")
                cpu_time, memory_use = time_cmd(cmd)
                ts_simplified = msprime.load(ts_out.name)
            return ts_simplified, cpu_time, memory_use
        except ValueError as e:
            # temporary hack around tsinfer bug
            if 'time[parent] must be greater than time[child]' in str(e):
                logging.info("Hit tsinfer bug. Skipping")
                return None, None, None
            else:
                raise
    
    @staticmethod
    def run_fastarg(file_name, seq_length, seed):
        with tempfile.NamedTemporaryFile("w+") as fa_out, \
                tempfile.NamedTemporaryFile("w+") as tree, \
                tempfile.NamedTemporaryFile("w+") as muts:
            cmd = msprime_fastARG.get_cmd(fastARG_executable, file_name, seed)
            cpu_time, memory_use = time_cmd(cmd, fa_out)
            logging.debug("ran fastarg for seq length {} [{} s]: '{}'".format(seq_length, cpu_time, cmd))
            var_pos = msprime_fastARG.variant_positions_from_fastARGin_name(file_name)
            inferred_ts = msprime_fastARG.fastARG_out_to_msprime(fa_out, var_pos, seq_len=seq_length)
            #check fastARG conversion give same haplotypes
            assert msprime_fastARG.compare_fastARG_haplotypes(file_name, inferred_ts, save=True)
            return inferred_ts, cpu_time, memory_use

    @staticmethod
    def run_rentplus(file_name, seq_length):
        """
        runs RentPlus, returning the output filename, the total CPU, & max mem
        must check here if we are using 0..1 positions (infinite sites) or integers
        """
        haplotype_lines = 0
        integer_positions = True
        with open(file_name, "r+") as infile:
            for pos in next(infile).split():
                try:
                    dummy = int(pos)
                except ValueError:
                    integer_positions = False
            for line in infile:
                if line.rstrip():
                    haplotype_lines += 1
        cmd = ["java", "-jar", RentPlus_executable]
        cmd += [file_name] if integer_positions else ['-l', seq_length, file_name]
        with tempfile.NamedTemporaryFile("w+") as script_output:
            cpu_time, memory_use = time_cmd(cmd, script_output)
        logging.debug("ran RentPlus for {} haplotypes with seq length {} [{} s]: '{}'".format(
            haplotype_lines, seq_length, cpu_time, cmd))
        #we cannot back-convert RentPlus output to treeseq form - just return the trees file
        assert os.path.isfile(file_name + ".trees"), 'No trees file created when running Rent+'
        #we might also want to look at TMRCAs (file_name + '.Tmrcas')
        return file_name + ".trees", haplotype_lines, cpu_time, memory_use

    @staticmethod
    def run_argweaver(
            sites_file, Ne, recombination_rate, mutation_rate, path_prefix, seed,
            MSMC_samples, sample_step, burnin_iterations, ntimes, verbose=False):
        """
        this produces a whole load of .smc files labelled <path_prefix>i.0.smc,
        <path_prefix>i.10.smc, etc.

        Returns the iteration numbers ('0', '10', '20' etc), the name of the
        statistics file, the total CPU time, and the max memory usage.
        """
        cpu_time = []
        memory_use = []
        burn_prefix = None
        try:
            exe = [ARGweaver_executable, '--sites', sites_file.name if hasattr(sites_file, "name") else sites_file,
                   '--popsize', str(Ne),
                   '--recombrate', str(recombination_rate),
                   '--mutrate', str(mutation_rate),
                   '--ntimes', str(ntimes),
                   '--overwrite']
            if not verbose:
                exe += ['--quiet']
            if seed is not None:
                exe += ['--randseed', str(int(seed))]
            if burnin_iterations > 0:
                burn_in = str(int(burnin_iterations))
                burn_prefix = path_prefix+"_burn"
                logging.info("== Burning in ARGweaver MCMC using {} steps ==".format(burn_in))
                logging.debug("== ARGweaver burnin command is {} ==".format(" ".join(exe)))
                c, m = time_cmd(exe + ['--iters', burn_in,
                                       '--sample-step', burn_in,
                                       '--output', burn_prefix])
                cpu_time.append(c)
                memory_use.append(m)
                #if burn_in, read from the burn in arg file, rather than the original .sites
                exe += ['--arg', burn_prefix+"."+ burn_in +".smc.gz"]
            else:
                exe += ['--sites', sites_file]

            new_prefix = path_prefix + "_i" #we append a '_i' to mark iteration number
            iterations = int(sample_step * (MSMC_samples-1))
            exe += ['--output', new_prefix]
            exe += ['--iters', str(iterations)]
            exe += ['--sample-step', str(int(sample_step))]
            logging.info("== Running ARGweaver for {} steps to collect {} samples ==".format( \
                int(iterations), MSMC_samples))
            logging.debug("== ARGweaver command is {} ==".format(" ".join(exe)))
            c, m = time_cmd(exe)
            cpu_time.append(c)
            memory_use.append(m)

            smc_prefix = new_prefix + "." #the arg-sample program adds .iteration_num
            saved_iterations = [f[len(smc_prefix):-7] for f in glob.glob(smc_prefix + "*" + ".smc.gz")]
            new_stats_file_name = path_prefix+".stats"

            #concatenate all the stats together
            with open(new_stats_file_name, "w+") as stats:
                if burn_prefix:
                    shutil.copyfileobj(open(burn_prefix + ".stats"), stats)
                    print("\n", file=stats)
                shutil.copyfileobj(open(new_prefix + ".stats"), stats)
            #cannot translate these to msprime ts objects, as smc2arg does not work
            #see https://github.com/mdrasmus/argweaver/issues/20
            return saved_iterations, new_stats_file_name, sum(cpu_time), max(memory_use)
        except ValueError as e:
            if 'src/argweaver/sample_thread.cpp:517:' in str(e):
                logging.info("Hit argweaver bug https://github.com/mcveanlab/treeseq-inference/issues/25. Skipping")
                return [], "NA", None, None
            elif "Assertion `trans[path[i]] != 0.0' failed" in str(e):
                logging.info("Hit argweaver bug https://github.com/mcveanlab/treeseq-inference/issues/42. Skipping")
                return [], "NA", None, None
            else:
                raise


def infer_worker(work):
    """
    Entry point for running a single inference task in a worker process.
    """
    tool, row, simulations_dir, num_threads, compute_metrics = work
    runner = InferenceRunner(tool, row, simulations_dir, num_threads, compute_metrics)
    result = runner.run()
    result['completed'] = True
    return int(row[0]), tool, result


class Dataset(object):
    """
    A dataset is some collection of simulations and associated data.
    """
    name = None
    """
    Each dataset has a unique name. This is used as the prefix for the data
    file and raw_data_dir directory. Within this, replicate instances of datasets
    (each with a different RNG seed) are saved under the seed number
    """
    compute_tree_metrics = False
    """
    Set to true if we wish to compute tree metric distances from the source
    tree sequence to the inferred tree sequence(s).
    """

    data_dir = "data"

    tools = [
        ARGWEAVER,
        FASTARG,
        RENTPLUS,
        TSINFER,
    ]

    """
    These are the basic columns that record the simulation used (can be overridden)
    """
    sim_cols = [
        "sample_size", "Ne", "length", "recombination_rate", "mutation_rate",
        "error_rate", "seed"]

    #for a tidier csv file, we can exclude any of the save_stats values or ARGmetrics columns
    exclude_colnames = []

    def __init__(self):
        self.data_path = os.path.abspath(
            os.path.join(self.data_dir, "{}".format(self.name)))
        self.data_file = self.data_path + ".csv"
        self.param_file = self.data_path + ".json"
        self.raw_data_dir = os.path.join(self.data_dir, "raw__NOBACKUP__", self.name)
        self.simulations_dir = os.path.join(self.raw_data_dir, "simulations")
        self.last_data_write_time = time.time()

    def load_data(self):
        self.data = pd.read_csv(self.data_file)

    def dump_data(self, write_index=False, force_flush=True):
        """
        Dumps data if it hasn't been written in the last 30 seconds. If force is true,
        write it out anyway.
        """
        now = time.time()
        if force_flush or (now - self.last_data_write_time) > 30:
            logging.info("Flushing data file")
            self.data.to_csv(self.data_file, index=write_index)
            self.last_data_write_time = time.time()

    #
    # Main entry points.
    #
    def setup(self, args):
        """
        Creates the dataframe and storage directories and then runs the initial
        simulations.
        """
        if os.path.exists(self.simulations_dir):
            shutil.rmtree(self.simulations_dir)
            logging.info("Deleting dir {}".format(self.simulations_dir))
        os.makedirs(self.simulations_dir)
        self.verbosity = args.verbosity
        logging.info("Creating dir {}".format(self.simulations_dir))
        self.data = self.run_simulations(args.replicates, args.seed, args.progress)
        for t in self.tools:
            col = t + "_completed"
            if col not in self.data:
                self.data[col] = False
        # Other result columns are added later during the infer step.
        self.dump_data(write_index=True)

    def infer(
            self, num_processes, num_threads, force=False, specific_tool=None,
            specific_row=None, flush_all=False, show_progress=False):
        """
        Runs the main inference processes and stores results in the dataframe.
        can 'force' all rows to be (re)run, or specify a specific row to run.
        """
        self.load_data()
        tools = self.tools
        if specific_tool is not None:
            if specific_tool not in self.tools:
                raise ValueError("Tool '{}' not recognised: options = {}".format(
                    specific_tool, list(self.tools)))
            tools = [specific_tool]
        row_ids = self.data.index
        if specific_row is not None:
            if specific_row < 0 or specific_row > len(self.data.index):
                raise ValueError("Row {} out of bounds".format(specific_row))
            row_ids = [specific_row]
        work = []
        tool_work_total = {tool: 0 for tool in tools}
        for row_id in row_ids:
            row = self.data.iloc[row_id]
            for tool in tools:
                #special case for ARGweaver, to allow multiple runs for the same sim
                if getattr(row,"only_AW",False) and tool != ARGWEAVER:
                    logging.info("Data row {} is set to skip {} inference".format(
                        row_id,tool) + " (probably to avoid duplicate effort)")
                    continue
                # Only run for a particular tool if it has not already completed
                # of if --force is specified.
                if force or not row[tool + "_completed"]:
                    work.append((
                        tool, row, self.simulations_dir, num_threads,
                        self.compute_tree_metrics))
                    tool_work_total[tool] += 1
                else:
                    logging.info(
                        "Data row {} is filled out for {} inference: skipping".format(
                            row_id, tool))
        logging.info(
            "running {} inference trials (max {} tools over {} of {} rows) with {} "
            "processes and {} threads".format(
                len(work), len(tools), int(np.ceil(len(work)/len(self.tools))),
                len(self.data.index), num_processes, num_threads))

        # Randomise the order that work is done in so that we get results for all parts
        # of the plots through rather than get complete results for the initial data
        # points.
        random.shuffle(work)
        tool_work_completed = {tool: 0 for tool in tools}
        if show_progress:
            width = max(len(tool) for tool in tools)
            progress = {
                tool:tqdm.tqdm(
                    desc="{:>{}}".format(tool, width),
                    total=tool_work_total[tool]) for tool in tools}

        def store_result(row_id, tool, results):
            tool_work_completed[tool] += 1
            logging.info("Inference {}/{} completed for {}".format(
                tool_work_completed[tool], tool_work_total[tool], tool))
            for k, v in results.items():
                if k not in self.exclude_colnames:
                    self.data.ix[row_id, tool + "_" + k] = v
            self.dump_data(force_flush=flush_all)
            # Update the progress meters
            if show_progress:
                progress[tool].update()

        if num_processes > 1:
            with multiprocessing.Pool(processes=num_processes) as pool:
                for result in pool.imap_unordered(infer_worker, work):
                    store_result(*result)
        else:
            # When we have only one process it's easier to keep everything in the same
            # process for debugging.
            for result in map(infer_worker, work):
                store_result(*result)
        self.dump_data(force_flush=True)

    #
    # Utilities for running simulations and saving files.
    #
    def single_simulation(self, n, Ne, l, rho, mu, seed, mut_seed=None,
        discretise_mutations=True):
        """
        The standard way to run one msprime simulation for a set of parameter
        values. Saves the output to an .hdf5 file, and also saves variant files
        for use in fastARG (a .hap file, in the format specified by
        https://github.com/lh3/fastARG#input-format) ARGweaver (a .sites file:
        http://mdrasmus.github.io/argweaver/doc/#sec-file-sites) tsinfer
        (currently a numpy array containing the variant matrix)

        mutation_seed is not yet implemented, but should allow the same
        ancestry to be simulated (if the same genealogy_seed is given) but have
        different mutations thrown onto the trees (even with different
        mutation_rates)

        Returns a tuple of treesequence, filename (without file type extension)
        """
        logging.info(
            "Running simulation for "
            "n = {}, l = {:.2g}, Ne = {}, rho = {}, mu = {}".format(
                n, l, Ne, rho, mu))
        # Since we want to have a finite site model, we force the recombination map
        # to have exactly l loci with a recombination rate of rho between them.
        recombination_map = msprime.RecombinationMap.uniform_map(l, rho, l)
        # We need to rejection sample any instances that we can't discretise under
        # the current model. The simplest way to do this is to have a local RNG
        # which we seed with the specified seed.
        rng = random.Random(seed)
        # TODO replace this with a proper finite site mutation model in msprime.
        done = False
        while not done:
            sim_seed = rng.randint(1, 2**31)
            ts = msprime.simulate(
                n, Ne, recombination_map=recombination_map, mutation_rate=mu,
                random_seed=sim_seed)
            if discretise_mutations:
                try:
                    ts = msprime_extras.discretise_mutations(ts)
                    done = True
                except ValueError as ve:
                    logging.info("Rejecting simulation: seed={}: {}".format(sim_seed, ve))
            else:
                done=True
        logging.info(
            "Simulation done; {} sites and {} trees".format(ts.num_sites, ts.num_trees))
        # Here we might want to iterate over mutation rates for the same
        # genealogy, setting a different mut_seed so that we can see for
        # ourselves the effect of mutation rate variation on a single topology
        # but for the moment, we don't bother, and simply write
        # mut_seed==genealogy_seed
        sim_fn = msprime_name(n, Ne, l, rho, mu, seed, seed, self.simulations_dir)
        logging.debug("writing {}.hdf5".format(sim_fn))
        ts.dump(sim_fn+".hdf5")
        return ts, sim_fn

    def single_simulation_with_selection(self, n, Ne, l, rho, mu, s, seed, mut_seed=None,
        discretise_mutations=True):
        """
        Run a forward simulation with selection for a set of parameter values. 
        using simuPOP. Convert the output to an .hdf5 file using ftprime. Other
        details as for single_simulation()
        Returns a tuple of treesequence, filename (without file type extension)
        """
        logging.info(
            "Running simulation with selection for "
            "n = {}, l = {:.2g}, Ne = {}, rho = {}, mu = {}, s = {}".format(
                n, l, Ne, rho, mu, s))
        cmd = [sys.executable, ftprime_singlelocus_executable,
            "--popsize",  Ne,
            "--recomb_rate", rho,
            "--length", l,
            "--neut_mut_rate", mu,
            "--nsamples", int(n/2), #this is # of diploid samples, so needs halving
            
            ]
        logging.info(
            "Simulation done; {} sites and {} trees".format(ts.num_sites, ts.num_trees))
        # Here we might want to iterate over mutation rates for the same
        # genealogy, setting a different mut_seed so that we can see for
        # ourselves the effect of mutation rate variation on a single topology
        # but for the moment, we don't bother, and simply write
        # mut_seed==genealogy_seed
        sim_fn = msprime_name(n, Ne, l, rho, mu, seed, seed, self.simulations_dir)
        logging.debug("writing {}.hdf5".format(sim_fn))
        ts.dump(sim_fn+".hdf5")
        return ts, sim_fn



    def save_variant_matrices(self, ts, fname, error_rate=0, infinite_sites=True):
        if infinite_sites:
            #for infinite sites, assume we have discretised mutations to ints
            if not all(p.is_integer() for p in pos):
                raise ValueError("Variant positions are not all integers")
        S = generate_samples(ts, error_rate)
        filename = add_error_param_to_name(fname, error_rate)
        if TSINFER in self.tools:
            outfile = filename + ".npy"
            logging.debug("writing variant matrix to {} for tsinfer".format(outfile))
            np.save(outfile, S)
            outfile = filename + ".pos.npy"
            pos = np.array([v.position for v in ts.variants()])
            logging.debug("writing variant positions to {} for tsinfer".format(outfile))
            np.save(outfile, pos)
        if FASTARG in self.tools:
            logging.debug("writing variant matrix to {}.hap for fastARG".format(filename))
            with open(filename+".hap", "w+") as fastarg_in:
                msprime_fastARG.variant_matrix_to_fastARG_in(S.T, pos, fastarg_in)
        if ARGWEAVER in self.tools:
            logging.debug("writing variant matrix to {}.sites for ARGweaver".format(filename))
            with open(filename+".sites", "w+") as argweaver_in:
                msprime_ARGweaver.variant_matrix_to_ARGweaver_in(
                    S.T, pos, ts.get_sequence_length(), argweaver_in,
                    infinite_sites=infinite_sites)
        if RENTPLUS in self.tools:
            logging.debug("writing variant matrix to {}.dat for RentPlus".format(filename))
            with open(filename+".dat", "wb+") as rentplus_in:
                msprime_RentPlus.variant_matrix_to_RentPlus_in(S.T, pos,
                        ts.get_sequence_length(), rentplus_in, infinite_sites=infinite_sites)


class MetricsByMutationRateDataset(Dataset):
    """
    Accuracy of ARG inference (measured by various statistics)
    tending to fully accurate as mutation rate increases
    """
    name = "metrics_by_mutation_rate"

    default_replicates = 10
    default_seed = 123
    compute_tree_metrics = True

    #for a tidier csv file, we can exclude any of the save_stats values or ARGmetrics columns
    exclude_colnames =[]


    def run_simulations(self, replicates, seed, show_progress):
        if replicates is None:
            replicates = self.default_replicates
        if seed is None:
            seed = self.default_seed
        rng = random.Random(seed)
        # Variable parameters
        mutation_rates = np.logspace(-8, -5, num=6)[:-1] * 1.5
        error_rates = [0, 0.01]
        sample_sizes = [10, 20]

        # Fixed parameters
        Ne = 5000
        length = 10000
        recombination_rate = 2.5e-8
        num_rows = replicates * len(mutation_rates) * len(error_rates) * len(sample_sizes)
        data = pd.DataFrame(index=np.arange(0, num_rows), columns=self.sim_cols)
        row_id = 0
        if show_progress:
            progress = tqdm.tqdm(total=num_rows)
        for replicate in range(replicates):
            for mutation_rate in mutation_rates:
                for sample_size in sample_sizes:
                    done = False
                    while not done:
                        replicate_seed = rng.randint(1, 2**31)
                        # Run the simulation
                        ts, fn = self.single_simulation(
                            sample_size, Ne, length, recombination_rate, mutation_rate,
                            replicate_seed, replicate_seed,
                            #discretise_mutations=True)
                            discretise_mutations=False) #stop doing Jerome's discretising step!
                        # Reject this instances if we got no mutations.
                        done = ts.get_num_mutations() > 0
                    with open(fn +".nex", "w+") as out:
                        ts.write_nexus_trees(
                            out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
                    # Add the rows for each of the error rates in this replicate
                    for error_rate in error_rates:
                        row = data.iloc[row_id]
                        row_id += 1
                        row.sample_size = sample_size
                        row.recombination_rate = recombination_rate
                        row.mutation_rate = mutation_rate
                        row.length = length
                        row.Ne = Ne
                        row.seed = replicate_seed
                        row.error_rate = error_rate
                        self.save_variant_matrices(ts, fn, error_rate,
                            #infinite_sites=True)
                            infinite_sites=False)
                        if show_progress:
                            progress.update()
        return data

class MetricsBySampleSizeDataset(Dataset):
    """
    Accuracy of ARG inference of a fixed subset of samples (measured by various statistics)
    as the population sample size increases. We also want to see what difference SRBs makes
    """
    name = "metrics_by_sample_size"
    tools = [TSINFER]
    default_replicates = 1
    default_seed = 123
    compute_tree_metrics = True

    #for a tidier csv file, we can exclude any of the save_stats values or ARGmetrics columns
    exclude_colnames =[]


    def run_simulations(self, replicates, seed, show_progress):
        if replicates is None:
            replicates = self.default_replicates
        if seed is None:
            seed = self.default_seed
        rng = random.Random(seed)

        # Variable parameters
        lengths = [10000, 100000, 1000000]
        error_rates = [0]
        sample_sizes = [12, 50, 100, 500, 1000]
        shared_breakpoint_params = [False, True]
        shared_length_params = [False, True]

        # Fixed parameters
        subsample_size=10
        assert subsample_size <= min(sample_sizes)
        Ne = 5000
        mutation_rate = 2.5e-8
        recombination_rate = 2.5e-8
        num_rows = replicates * len(lengths) * len(error_rates) * len(sample_sizes)
        cols = self.sim_cols + ["tsinfer_srb", "tsinfer_sl", "subsample_size"]
        data = pd.DataFrame(index=np.arange(0, num_rows), columns=cols)
        row_id = 0
        if show_progress:
            progress = tqdm.tqdm(total=num_rows)
        for replicate in range(replicates):
            for length in lengths:
                done = False
                while not done:
                    replicate_seed = rng.randint(1, 2**31)
                    # Run the simulation
                    base_ts, unused_fn = self.single_simulation(
                        max(sample_sizes), Ne, length, recombination_rate, mutation_rate,
                        replicate_seed, replicate_seed,
                        #discretise_mutations=True)
                        discretise_mutations=False) #stop doing Jerome's discretising step!
                    # Reject this instances if we got no mutations.
                    done = base_ts.get_num_mutations() > 0
                #Take the same base simulation and sample down to get comparable test sets
                for sample_size in sample_sizes:
                    ts = base_ts.simplify(list(range(sample_size)))
                    fn = msprime_name(sample_size, Ne, length, recombination_rate, mutation_rate, 
                        replicate_seed, replicate_seed, self.simulations_dir)
                    #subsample to produce a nexus file for metric comparison
                    subsampled_fn=add_subsample_param_to_name(fn, subsample_size)
                    subsampled_ts = ts.simplify(list(range(subsample_size)))
                    with open(subsampled_fn +".nex", "w+") as out:
                        subsampled_ts.write_nexus_trees(
                            out, zero_based_tip_numbers= tree_tip_labels_start_at_0)
                    # Add the rows for each of the error rates in this replicate
                    for tsinfer_srb in shared_breakpoint_params:
                        for tsinfer_sl in shared_length_params:
                            for error_rate in error_rates:
                                row = data.iloc[row_id]
                                row_id += 1
                                row.sample_size = sample_size
                                row.recombination_rate = recombination_rate
                                row.mutation_rate = mutation_rate
                                row.length = length
                                row.Ne = Ne
                                row.tsinfer_srb = tsinfer_srb
                                row.tsinfer_sl = tsinfer_sl
                                row.subsample_size = subsample_size
                                row.seed = replicate_seed
                                row.error_rate = error_rate
                                self.save_variant_matrices(ts, fn, error_rate,
                                    #infinite_sites=True)
                                    infinite_sites=False)
                                if show_progress:
                                    progress.update()
        return data

class MetricsByMutationRateWithSelectionDataset(MetricsByMutationRateDataset):
    """
    Accuracy of ARG inference (measured by various statistics)
    tending to fully accurate as mutation rate increases
    """
    name = "metrics_by_mutation_rate_with_selection"

    def run_simulations(self, replicates, seed, show_progress):
        if replicates is None:
            replicates = self.default_replicates
        if seed is None:
            seed = self.default_seed
        rng = random.Random(seed)
        # Variable parameters
        mutation_rates = np.logspace(-8, -5, num=6)[:-1] * 1.5
        error_rates = [0, 0.01]
        sample_sizes = [10, 20]

        # Fixed parameters
        Ne = 5000
        length = 10000
        recombination_rate = 2.5e-8
        selection_coefficient = 0.1
        num_rows = replicates * len(mutation_rates) * len(error_rates) * len(sample_sizes)
        data = pd.DataFrame(index=np.arange(0, num_rows), columns= self.sim_cols)
        row_id = 0
        if show_progress:
            progress = tqdm.tqdm(total=num_rows)
        for replicate in range(replicates):
            for mutation_rate in mutation_rates:
                for sample_size in sample_sizes:
                    done = False
                    while not done:
                        replicate_seed = rng.randint(1, 2**31)
                        # Run the simulation
                        ts, fn = self.single_simulation(
                            sample_size, Ne, length, recombination_rate, mutation_rate,
                            replicate_seed, replicate_seed,
                            #discretise_mutations=True)
                            discretise_mutations=False) #stop doing Jerome's discretising step!
                        # Reject this instances if we got no mutations.
                        done = ts.get_num_mutations() > 0
                    with open(fn +".nex", "w+") as out:
                        ts.write_nexus_trees(
                            out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
                    # Add the rows for each of the error rates in this replicate
                    for error_rate in error_rates:
                        row = data.iloc[row_id]
                        row_id += 1
                        row.sample_size = sample_size
                        row.recombination_rate = recombination_rate
                        row.mutation_rate = mutation_rate
                        row.length = length
                        row.Ne = Ne
                        row.seed = replicate_seed
                        row.error_rate = error_rate
                        self.save_variant_matrices(ts, fn, error_rate,
                            #infinite_sites=True)
                            infinite_sites=False)
                        if show_progress:
                            progress.update()
        return data

class TsinferPerformance(Dataset):
    """
    The performance of tsinfer in terms of CPU time, memory usage and
    compression rates for large scale datasets. Contains data for
    the two main dimensions of sample size and sequence length.
    """
    name = "tsinfer_performance"
    compute_tree_metrics = False
    default_replicates = 10
    default_seed = 123
    tools = [TSINFER]
    fixed_length = 5 * 10**6
    fixed_sample_size = 5000

    def run_simulations(self, replicates, seed, show_progress):
        # TODO abstract some of this functionality up into the superclass.
        # There is quite a lot shared with the other dataset.
        if replicates is None:
            replicates = self.default_replicates
        if seed is None:
            seed = self.default_seed
        rng = random.Random(seed)
        # Variable parameters
        num_points = 20
        sample_sizes = np.linspace(10, 2 * self.fixed_sample_size, num_points).astype(int)
        lengths = np.linspace(self.fixed_length / 10, 2 * self.fixed_length, num_points).astype(int)
        shared_breakpoint_params = [False, True]
        shared_length_params = [False, True]
        # Fixed parameters
        Ne = 5000
        # TODO we'll want to do this for multiple error rates eventually. For now
        # just look at perfect data.
        error_rate = 0
        recombination_rate = 2.5e-8
        mutation_rate = recombination_rate
        num_rows = 2 * num_points * replicates * len(shared_breakpoint_params) * len(shared_length_params)
        cols = self.sim_cols + ["edges", "ts_filesize", "tsinfer_srb", "tsinfer_sl"]
        data = pd.DataFrame(index=np.arange(0, num_rows), columns=cols)
        work = [
            (self.fixed_sample_size, l) for l in lengths] + [
            (n, self.fixed_length) for n in sample_sizes]
        row_id = 0
        if show_progress:
            progress = tqdm.tqdm(total=num_rows)
        for sample_size, length in work:
            for _ in range(replicates):
                replicate_seed = rng.randint(1, 2**31)
                #use same simulation seed for different params
                ts, fn = self.single_simulation(
                    sample_size, Ne, length, recombination_rate, mutation_rate,
                    replicate_seed, replicate_seed, discretise_mutations=False)
                assert ts.get_num_mutations() > 0
                # Tsinfer should be robust to this, but it currently isn't. Fail
                # noisily now rather than obscurely later. This will only ever happen
                # in trivially small data sets, so it doesn't matter.
                non_singletons = False
                for v in ts.variants():
                    if np.sum(v.genotypes) > 1:
                        non_singletons = True
                if not non_singletons:
                    raise ValueError("No non-single mutations present")
                for tsinfer_srb in shared_breakpoint_params:
                    for tsinfer_sl in shared_length_params:
                        row = data.iloc[row_id]
                        row_id += 1
                        row.sample_size = sample_size
                        row.recombination_rate = recombination_rate
                        row.mutation_rate = mutation_rate
                        row.length = length
                        row.Ne = Ne
                        row.seed = replicate_seed
                        row.error_rate = error_rate
                        row.tsinfer_srb = tsinfer_srb
                        row.tsinfer_sl = tsinfer_sl
                        row.ts_filesize = os.path.getsize(fn + ".hdf5")
                        row.edges = ts.num_edges
                        self.save_variant_matrices(ts, fn, error_rate, infinite_sites=False)
                        if show_progress:
                            progress.update()
        return data


class ProgramComparison(Dataset):
    """
    The performance of the various programs in terms of running time.
    """
    name = "program_comparison"
    compute_tree_metrics = False
    default_replicates = 2
    default_seed = 1000
    tools = [FASTARG, TSINFER]
    fixed_length = 5 * 10**6
    fixed_sample_size = 10000

    def run_simulations(self, replicates, seed, show_progress):
        # TODO abstract some of this functionality up into the superclass.
        # There is quite a lot shared with the other dataset.
        if replicates is None:
            replicates = self.default_replicates
        if seed is None:
            seed = self.default_seed
        rng = random.Random(seed)

        # Variable parameters
        num_points = 20
        sample_sizes = np.linspace(10, 2 * self.fixed_sample_size, num_points).astype(int)
        lengths = np.linspace(self.fixed_length / 10, 2 * self.fixed_length, num_points).astype(int)

        # Fixed parameters
        Ne = 5000
        error_rate = 0
        recombination_rate = 2.5e-8
        mutation_rate = recombination_rate
        num_rows = 2 * num_points * replicates
        cols = self.sim_cols + [tool + pf for tool in self.tools for pf in ("_cputime", "_memory")]
        data = pd.DataFrame(index=np.arange(0, num_rows), columns=cols)
        work = [
            (self.fixed_sample_size, l) for l in lengths] + [
            (n, self.fixed_length) for n in sample_sizes]
        row_id = 0
        if show_progress:
            progress = tqdm.tqdm(total=num_rows)
        for sample_size, length in work:
            for _ in range(replicates):
                replicate_seed = rng.randint(1, 2**31)
                ts, fn = self.single_simulation(
                    sample_size, Ne, length, recombination_rate, mutation_rate,
                    replicate_seed, replicate_seed, discretise_mutations=False)
                assert ts.get_num_mutations() > 0
                # Tsinfer should be robust to this, but it currently isn't. Fail
                # noisily now rather than obscurely later. This will only ever happen
                # in trivially small data sets, so it doesn't matter.
                non_singletons = False
                for v in ts.variants():
                    if np.sum(v.genotypes) > 1:
                        non_singletons = True
                if not non_singletons:
                    raise ValueError("No non-single mutations present")
                row = data.iloc[row_id]
                row_id += 1
                row.sample_size = sample_size
                row.recombination_rate = recombination_rate
                row.mutation_rate = mutation_rate
                row.length = length
                row.Ne = Ne
                row.seed = replicate_seed
                row.error_rate = 0.0
                # for tool in self.tools:
                #     row[tool + "_completed"] = False
                # # Hack to prevent RentPlus from running when the sizes are too big.
                # if sample_size == self.fixed_sample_size:
                #     if length > lengths[0]:
                #         row.RentPlus_completed = True
                # else:
                #     if sample_size > sample_sizes[0]:
                #         row.RentPlus_completed = True

                self.save_variant_matrices(ts, fn, error_rate, infinite_sites=False)
                if show_progress:
                    progress.update()
        return data

class ARGweaverParamChanges(Dataset):
    """
    Accuracy of ARGweaver inference (measured by various tree statistics)
    as we adjust burn-in time and number of time slices.
    This is an attempt to explain why ARGweaver can do badly e.g. for high mutation rates

    You can check that the timeslices really *are* having an effect by looking at the unique
    times (field 3) in the .arg files within raw__NOBACKUP__/argweaver_param_changes e.g. 
    cut -f 3 data/raw__NOBACKUP__/argweaver_param_changes/simulations/<filename>.arg | sort | uniq
    """
    name = "argweaver_param_changes"
    tools = [ARGWEAVER, TSINFER]
    default_replicates = 40
    default_seed = 123
    compute_tree_metrics = True

    #for a tidier csv file, we can exclude any of the save_stats values or ARGmetrics columns
    exclude_colnames =[]


    def run_simulations(self, replicates, seed, show_progress):
        if replicates is None:
            replicates = self.default_replicates
        if seed is None:
            seed = self.default_seed
        rng = random.Random(seed)
        # Variable parameters
        mutation_rates = np.logspace(-8, -3, num=8)[:-1] * 1.5
        error_rates = [0]
        sample_sizes = [6]
        AW_burnin_steps = [1000,2000,5000] #by default, bin/arg-sim has no burnin time
        AW_num_timepoints = [20,60,200] #by default, bin/arg-sim has n=20
        # Fixed parameters
        Ne = 5000
        length = 10000
        recombination_rate = 2.5e-8
        num_rows = replicates * len(mutation_rates) * len(error_rates) * len(sample_sizes) * \
            len(AW_burnin_steps) * len(AW_num_timepoints)
        cols = self.sim_cols + ["only_AW", "ARGweaver_burnin", "ARGweaver_ntimes"]
        data = pd.DataFrame(index=np.arange(0, num_rows), columns=cols)
        row_id = 0
        if show_progress:
            progress = tqdm.tqdm(total=num_rows)
        for replicate in range(replicates):
            for mutation_rate in mutation_rates:
                for sample_size in sample_sizes:
                    done = False
                    while not done:
                        replicate_seed = rng.randint(1, 2**31)
                        # Run the simulation
                        ts, fn = self.single_simulation(
                            sample_size, Ne, length, recombination_rate, mutation_rate,
                            replicate_seed, replicate_seed,
                            #discretise_mutations=True)
                            discretise_mutations=False) #stop doing Jerome's discretising step!
                        # Reject this instances if we got no mutations.
                        done = ts.get_num_mutations() > 0
                    with open(fn +".nex", "w+") as out:
                        ts.write_nexus_trees(
                            out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
                    # Add the rows for each of the error rates in this replicate
                    for error_rate in error_rates:
                        only_run_ARGweaver_inference = 0
                        #set up new rows for each set of ARGweaver parameters
                        for burnin in AW_burnin_steps:
                            for n_timesteps in AW_num_timepoints:
                                row = data.iloc[row_id]
                                row_id += 1
                                row.sample_size = sample_size
                                row.recombination_rate = recombination_rate
                                row.mutation_rate = mutation_rate
                                row.length = length
                                row.Ne = Ne
                                row.seed = replicate_seed
                                row.error_rate = error_rate
                                row.ARGweaver_burnin = burnin
                                row.ARGweaver_ntimes = n_timesteps
                                row.only_AW = only_run_ARGweaver_inference
                                #only run other infers for the first row in this set of AW run parameters
                                only_run_ARGweaver_inference = 1
                        self.save_variant_matrices(ts, fn, error_rate,
                            #infinite_sites=True)
                            infinite_sites=False)
                        if show_progress:
                            progress.update()
        return data


######################################
#
# Figures
#
######################################

class Figure(object):
    """
    Superclass of all figures. Each figure depends on a dataset.
    """
    datasetClass = None
    name = None
    figures_dir = "figures"
    tools_format = collections.OrderedDict([
        (TSINFER,   {"mark":"*", "col":"blue"}),
        (RENTPLUS,  {"mark":"s", "col":"red"}),
        (ARGWEAVER, {"mark":"o", "col":"green"}),
        (FASTARG,   {"mark":"^", "col":"magenta"}),
    ])
    """
    Each figure has a unique name. This is used as the identifier and the
    file name for the output plots.
    """

    def __init__(self):
        self.dataset = self.datasetClass()
        self.dataset.load_data()

    def savefig(self, figure):
        filename = os.path.join(self.figures_dir, "{}.pdf".format(self.name))
        figure.savefig(filename)

    def plot(self):
        raise NotImplementedError()


class AllMetricsByMutationRateFigure(Figure):
    """
    Simple figure that shows all the metrics at the same time.
    """
    datasetClass = MetricsByMutationRateDataset
    name = "all_metrics_by_mutation_rate"

    def plot(self):
        df = self.dataset.data
        error_rates = df.error_rate.unique()
        sample_sizes = df.sample_size.unique()

        metrics = ARG_metrics.get_metric_names()
        fig, axes = pyplot.subplots(len(metrics), 3, figsize=(12, 30))
        lines = []
        for j, metric in enumerate(metrics):
            for k, error_rate in enumerate(error_rates):
                ax = axes[j][k]
                if j == 0:
                    ax.set_title("Error = {}".format(error_rate))
                if k == 0:
                    ax.set_ylabel(metric + " metric")
                if j == len(metrics) - 1:
                    ax.set_xlabel("Mutation rate")
                for n, linestyle in zip(sample_sizes, ["-", "-."]):
                    df_s = df[np.logical_and(df.sample_size == n, df.error_rate == error_rate)]
                    group = df_s.groupby(["mutation_rate"])
                    group_mean = group.mean()
                    for tool, setting in self.tools_format.items():
                        ax.semilogx(
                            group_mean[tool + "_" + metric], linestyle, color=setting["col"])
                        # ax.plot(group_mean[tool + "_" + metric])

        self.savefig(fig)


class MetricByMutationRateFigure(Figure):
    """
    Superclass of the metric by mutations rate figure. Each subclass should be a
    single figure for a particular metric.
    """
    datasetClass = MetricsByMutationRateDataset


    def plot(self):
        df = self.dataset.data
        error_rates = df.error_rate.unique()
        sample_sizes = df.sample_size.unique()

        linestyles = ["-", ":"]
        fig, axes = pyplot.subplots(1, 3, figsize=(12, 6), sharey=True)
        lines = []
        for k, error_rate in enumerate(error_rates):
            ax = axes[k]
            ax.set_title("Error = {}".format(error_rate))
            ax.set_xlabel("Mutation rate")
            ax.set_xscale('log')
            if k == 0:
                ax.set_ylabel(self.metric + " metric")
            for n, linestyle in zip(sample_sizes, linestyles):
                df_s = df[np.logical_and(df.sample_size == n, df.error_rate == error_rate)]
                group = df_s.groupby(["mutation_rate"])
                mean_sem = [{'mu':g, 'mean':data.mean(), 'sem':data.sem()} for g, data in group]
                for tool,setting in self.tools_format.items():
                    if getattr(self, 'error_bars', None):
                        yerr=[m['sem'][tool + "_" + self.metric] for m in mean_sem]
                    else:
                        yerr = None
                    ax.errorbar(
                        [m['mu'] for m in mean_sem], 
                        [m['mean'][tool + "_" + self.metric] for m in mean_sem],
                        yerr=yerr,
                        linestyle=linestyle,
                        color=setting["col"],
                        marker=setting["mark"],
                        elinewidth=1)

        axes[0].set_ylim(self.ylim)

        # Create legends from custom artists
        artists = [
            pyplot.Line2D((0,1),(0,0), color= setting["col"],
                marker= setting["mark"], linestyle='')
            for tool,setting in self.tools_format.items()]
        first_legend = axes[0].legend(
            artists, self.tools_format.keys(), numpoints=3, loc="upper center")
            # bbox_to_anchor=(0.0, 0.1))
        # ax = pyplot.gca().add_artist(first_legend)
        artists = [
            pyplot.Line2D(
                (0,0),(0,0), color="black", linestyle=linestyle, linewidth=2)
            for linestyle in linestyles]
        axes[-1].legend(
            artists, ["Sample size = {}".format(n) for n in sample_sizes],
            loc="upper center")
        self.savefig(fig)


class RFRootedMetricByMutationsRateFigure(MetricByMutationRateFigure):
    name = "rf_rooted_by_mutation_rate"
    metric = "RFrooted"
    ylim = None


class KCRootedMetricByMutationsRateFigure(MetricByMutationRateFigure):
    name = "kc_rooted_by_mutation_rate"
    metric = "KCrooted"
    ylim = (0, 110)
    error_bars = True

class CputimeMetricByMutationsRateFigure(MetricByMutationRateFigure):
    """
    This figure is useful because we can only really get the CPU times
    for all four methods in the same scale for these tiny examples.
    We can show that ARGWeaver and RentPlus are much slower than tsinfer
    and FastARG here and compare tsinfer and FastARG more thoroughly
    in a dedicated figure.
    """
    name = "cputime_by_mutation_rate"

    def plot(self):
        df = self.dataset.data
        sample_sizes = df.sample_size.unique()

        linestyles = ["-", ":"]
        fig, ax = pyplot.subplots(1, 1)
        lines = []
        error_rate = 0
        n = 50
        ax.set_xlabel("Mutation rate")
        ax.set_ylabel("CPU time (sec)")
        df_s = df[np.logical_and(df.sample_size == n, df.error_rate == error_rate)]
        group = df_s.groupby(["mutation_rate"])
        group_mean = group.mean()
        for tool, setting in self.tools_format.items():
            ax.semilogx(
                group_mean[tool + "_" + "cputime"],
                color=setting["col"],
                marker=setting["mark"],
                label=tool)
        ax.legend(loc="center left")
        ax.set_ylim(-20, 1000)
        self.savefig(fig)

class MetricByARGweaverParametersFigure(Figure):
    """
    See the effect of burnin time and number of timeslices on the accuracy of ARGweaver
    as compared to TSinfer, when looking at tree metrics.
    """
    datasetClass = ARGweaverParamChanges

    def plot(self):
        df = self.dataset.data
        tools = self.dataset.tools
        AW_burnin = df.ARGweaver_burnin.unique()
        AW_discrete_timeslices = df.ARGweaver_ntimes.unique()

        # TODO move this into the superclass so that we have consistent styling.
        linestyles = [":","-.", "-"]
        fig, axes = pyplot.subplots(1, 3, figsize=(12, 6), sharey=True)
        lines = []
        for k, ntimes in enumerate(AW_discrete_timeslices):
            ax = axes[k]
            ax.set_title("ARGweaver timeslices = {}".format(ntimes))
            ax.set_xlabel("Mutation rate")
            ax.set_xscale('log')
            if k == 0:
                ax.set_ylabel(self.metric + " metric")
            
            #get the only tsinfer data for this selection (regardless of ntimes)
            tool = TSINFER
            df_s = df[np.logical_not(df[tool + "_" + self.metric].isnull())]
            group = df_s.groupby(["mutation_rate"])
            mean_sem = [{'mu':g, 'mean':data.mean(), 'sem':data.sem()} for g, data in group]
            if getattr(self, 'error_bars', None):
                yerr=[m['sem'][tool + "_" + self.metric] for m in mean_sem]
            else:
                yerr = None
            ax.errorbar(
                [m['mu'] for m in mean_sem], 
                [m['mean'][tool + "_" + self.metric] for m in mean_sem],
                yerr=yerr,
                linestyle=linestyles[0],
                color=self.tools_format[tool]["col"],
                marker=self.tools_format[tool]["mark"],
                elinewidth=1)
            
            
            for n, linestyle in zip(AW_burnin, linestyles):
                tool = ARGWEAVER
                df_s = df[np.logical_and(df.ARGweaver_burnin == n, df.ARGweaver_ntimes == ntimes)]
                group = df_s.groupby(["mutation_rate"])
                mean_sem = [{'mu':g, 'mean':data.mean(), 'sem':data.sem()} for g, data in group]
                if getattr(self, 'error_bars', None):
                    yerr=[m['sem'][tool + "_" + self.metric] for m in mean_sem]
                else:
                    yerr = None
                ax.errorbar(
                    [m['mu'] for m in mean_sem], 
                    [m['mean'][tool + "_" + self.metric] for m in mean_sem],
                    yerr=yerr,
                    linestyle=linestyle,
                    color=self.tools_format[tool]["col"],
                    marker=self.tools_format[tool]["mark"],
                    elinewidth=1)

        axes[0].set_ylim(self.ylim)

        # Create legends from custom artists
        artists = [
            pyplot.Line2D((0,1),(0,0), color=self.tools_format[tool]["col"],
                marker=self.tools_format[tool]["mark"], linestyle='')
            for tool in tools]
        first_legend = axes[0].legend(
            artists, tools, numpoints=3, loc="upper center")
            # bbox_to_anchor=(0.0, 0.1))
        # ax = pyplot.gca().add_artist(first_legend)
        artists = [
            pyplot.Line2D(
                (0,0),(0,0), color="black", linestyle=linestyle, linewidth=2)
            for linestyle in linestyles]
        axes[-1].legend(
            artists, ["Burn in = {} gens".format(n) for n in AW_burnin],
            loc="upper center")
        self.savefig(fig)

class KCRootedMetricByARGweaverParametersFigure(MetricByARGweaverParametersFigure):
    name = "kc_rooted_metrics_by_argweaver_params"
    metric = "KCrooted"
    ylim = (0, 4)
    error_bars = True

class RFRootedMetricByARGweaverParametersFigure(MetricByARGweaverParametersFigure):
    name = "rf_rooted_metrics_by_argweaver_params"
    metric = "RFrooted"
    ylim = None
    #ylim = (0, 4)
    error_bars = True


class PerformanceFigure(Figure):
    """
    Superclass for the performance metrics figures. Each of these figures
    has two panels; one for scaling by sequence length and the other
    for scaling by sample size. Different lines are given for each
    of the different combinations of tsinfer parameters
    """
    datasetClass = TsinferPerformance
    plotted_column = None
    y_axis_label = None

    def plot(self):
        df = self.dataset.data
        # Rescale the length to MB
        df.length /= 10**6
        # Set statistics to the ratio of observed over expected
        fig, (ax1, ax2) = pyplot.subplots(1, 2, sharey=True, figsize=(8, 5.5))
        source_colour = "red"
        inferred_colour = "blue"
        inferred_linestyles = {False:{False:':',True:'-.'},True:{False:'--',True:'-'}}
        inferred_markers =    {False:{False:':',True:'-.'},True:{False:'--',True:'-'}}
        fig, (ax1, ax2) = pyplot.subplots(1, 2, figsize=(12, 6), sharey=True)
        ax1.set_title("Fixed number of chromosomes ({})".format(self.datasetClass.fixed_sample_size))
        ax1.set_xlabel("Sequence length (MB)")
        ax1.set_ylabel(self.y_axis_label)
        for shared_breakpoint in [False,True]:
            for shared_length in [False, True]:
                dfp = df[np.logical_and.reduce((
                    df.sample_size == self.datasetClass.fixed_sample_size,
                    df.tsinfer_srb == shared_breakpoint,
                    df.tsinfer_sl == shared_length))]
                group = dfp.groupby(["length"])
                    #NB pandas.DataFrame.mean and pandas.DataFrame.sem have skipna=True by default
                mean_sem = [{'mu':g, 'mean':data.mean(), 'sem':data.sem()} for g, data in group]
                if getattr(self, 'error_bars', None):
                    yerr=[m['sem'] for m in mean_sem]
                else:
                    yerr = None
                ax1.errorbar(
                    [m['mu'] for m in mean_sem], 
                    [m['mean'][self.plotted_column] for m in mean_sem],
                    yerr=yerr,
                    linestyle=inferred_linestyles[shared_breakpoint][shared_length],
                    color=inferred_colour,
                    #marker=self.tools_format[tool]["mark"],
                    elinewidth=1)
                
        ax2.set_title("Fixed sequence length ({:.2f} Mb)".format(self.datasetClass.fixed_length / 10**6))
        ax2.set_xlabel("Sample size")
        ax2.set_ylabel(self.y_axis_label)
        for shared_breakpoint in [False,True]:
            for shared_length in [False, True]:
                dfp = df[np.logical_and.reduce((
                    df.length == self.datasetClass.fixed_length / 10**6,
                    df.tsinfer_srb == shared_breakpoint,
                    df.tsinfer_sl == shared_length))]
                group = dfp.groupby(["sample_size"])
                    #NB pandas.DataFrame.mean and pandas.DataFrame.sem have skipna=True by default
                mean_sem = [{'mu':g, 'mean':data.mean(), 'sem':data.sem()} for g, data in group]
                if getattr(self, 'error_bars', None):
                    yerr=[m['sem'] for m in mean_sem]
                else:
                    yerr = None
                ax2.errorbar(
                    [m['mu'] for m in mean_sem], 
                    [m['mean'][self.plotted_column] for m in mean_sem],
                    yerr=yerr,
                    linestyle=inferred_linestyles[shared_breakpoint][shared_length],
                    color=inferred_colour,
                    #marker=self.tools_format[tool]["mark"],
                    elinewidth=1)
        
        # ax1.plot(group_mean[self.plotted_column],
        #         color=source_colour, linestyle="-", label="Source")
        # ax1.plot(
        #     group_mean["tsinfer_" + self.plotted_column],
        #     color=inferred_colour, linestyle="-", label="Inferred")
        # ax1.legend(
        #     loc="upper left", numpoints=1, fontsize="small")

        # ax1.set_xlim(-5, 105)
        # ax1.set_ylim(-5, 250)
        # ax2.set_xlim(-5, 105)

        params = [
            pyplot.Line2D(
                (0,0),(0,0), color= inferred_colour, 
                linestyle=inferred_linestyles[shared_breakpoint][shared_length], linewidth=2)
            for shared_breakpoint, linestyles2 in inferred_linestyles.items()
            for shared_length, linestyle in linestyles2.items()]
        ax1.legend(
            params, ["breakpoints={}, lengths={}".format(srb, sl)
                for srb, linestyles2 in inferred_linestyles.items()
                for sl, linestyle in  linestyles2.items()],
            loc="lower right", fontsize=10)

        # fig.text(0.19, 0.97, "Sample size = 1000")
        # fig.text(0.60, 0.97, "Sequence length = 50Mb")
        # pyplot.savefig("plots/simulators.pdf")
        pyplot.suptitle('Tsinfer large dataset performance')
        self.savefig(fig)



class EdgesPerformanceFigure(PerformanceFigure):
    name = "edges_performance"
    plotted_column = "metric"
    y_axis_label = "inferred_edges / real_edges"
    def plot(self):
        self.dataset.data[self.plotted_column] = self.dataset.data["tsinfer_edges"] / self.dataset.data["edges"]
        PerformanceFigure.plot(self)

class FileSizePerformanceFigure(PerformanceFigure):
    name = "filesize_performance"
    plotted_column = "metric"
    y_axis_label = "inferred_filesize / real_filesize"
    y_axis_label = "File size relative to original"
    def plot(self):
        self.dataset.data[self.plotted_column] = self.dataset.data["tsinfer_ts_filesize"] / self.dataset.data["ts_filesize"]
        PerformanceFigure.plot(self)


class ProgramComparisonFigure(Figure):
    """
    Superclass for the program comparison figures. Each figure
    has two panels; one for scaling by sequence length and the other
    for scaling by sample size.
    """
    datasetClass = ProgramComparison

    def plot(self):
        df = self.dataset.data
        tools = self.dataset.tools

        # Rescale the length to MB
        length_scale = 10**6
        df.length /= length_scale
        # Scale time to hours
        time_scale = 3600
        for tool in tools:
            df[tool + "_cputime"] /= time_scale
        # Scale memory to GiB
        for tool in tools:
            df[tool + "_memory"] /= 1024 * 1024 * 1024

        fig, (ax1, ax2) = pyplot.subplots(1, 2, sharey=True, figsize=(8, 5.5))

        dfp = df[df.sample_size == self.datasetClass.fixed_sample_size]
        group = dfp.groupby(["length"])
        group_mean = group.mean()

        for tool in tools:
            col = tool + "_" + self.plotted_column
            ax1.plot(group_mean[col], label=tool, color=self.tools_format[tool]["col"])
        ax1.legend(
            loc="upper left", numpoints=1, fontsize="small")

        ax1.set_xlabel("Length (MB)")
        ax1.set_ylabel(self.y_label)

        dfp = df[df.length == self.datasetClass.fixed_length / length_scale]
        group = dfp.groupby(["sample_size"])
        group_mean = group.mean()

        for tool in tools:
            col = tool + "_" + self.plotted_column
            ax2.plot(group_mean[col], label=tool, color=self.tools_format[tool]["col"])

        ax2.set_xlabel("Sample size")

        # ax1.set_xlim(-5, 105)
        # ax1.set_ylim(-5, 250)
        # ax2.set_xlim(-5, 105)

        fig.tight_layout()

        # fig.text(0.19, 0.97, "Sample size = 1000")
        # fig.text(0.60, 0.97, "Sequence length = 50Mb")
        # pyplot.savefig("plots/simulators.pdf")
        self.savefig(fig)


class ProgramComparisonTimeFigure(ProgramComparisonFigure):
    name = "program_comparison_time"
    plotted_column = "cputime"
    y_label = "CPU time (hours)"

class ProgramComparisonMemoryFigure(ProgramComparisonFigure):
    name = "program_comparison_memory"
    plotted_column = "memory"
    y_label = "Memory (GiB)"

def run_setup(cls, args):
    f = cls()
    f.setup(args)

def run_infer(cls, args):
    logging.info("Inferring {}".format(cls.name))
    f = cls()
    f.infer(
        args.processes, args.threads, force=args.force, specific_tool=args.tool,
        specific_row=args.row, flush_all=args.flush_all, show_progress=args.progress)

def run_plot(cls, args):
    f = cls()
    f.plot()


def main():
    datasets = Dataset.__subclasses__()
    figures = [
        AllMetricsByMutationRateFigure,
        RFRootedMetricByMutationsRateFigure,
        KCRootedMetricByMutationsRateFigure,
        CputimeMetricByMutationsRateFigure,
        KCRootedMetricByARGweaverParametersFigure,
        RFRootedMetricByARGweaverParametersFigure,
        EdgesPerformanceFigure,
        FileSizePerformanceFigure,
        ProgramComparisonTimeFigure,
        ProgramComparisonMemoryFigure,
    ]
    name_map = dict([(d.name, d) for d in datasets + figures])
    parser = argparse.ArgumentParser(
        description="Set up base data, generate inferred datasets, process datasets and plot figures.")
    parser.add_argument('--verbosity', '-v', action='count', default=0)
    subparsers = parser.add_subparsers()
    subparsers.required = True
    subparsers.dest = 'command'

    subparser = subparsers.add_parser('setup')
    subparser.add_argument(
        'name', metavar='NAME', type=str, nargs=1,
        help='the dataset identifier', choices=[d.name for d in datasets])
    subparser.add_argument(
         '--replicates', '-r', type=int, help="number of replicates")
    subparser.add_argument(
         '--seed', '-s', type=int, help="use a non-default RNG seed")
    subparser.add_argument(
         '--hack_finite_sites', action='store_true',
         help="Mutations at the same (integer) location are superimposed, not shifted along")
    subparser.add_argument(
         '--progress',  "-P", action='store_true',
         help="Show a progress bar.", )
    subparser.set_defaults(func=run_setup)

    subparser = subparsers.add_parser('infer')
    subparser.add_argument(
        "--processes", '-p', type=int, default=1,
        help="number of worker processes, e.g. 40")
    subparser.add_argument(
        "--threads", '-t', type=int, default=1,
        help="number of threads per worker process (for supporting tools), e.g. 8")
    subparser.add_argument(
        "--tool", '-T', default=None,
        help="Only run this specific tool")
    subparser.add_argument(
        "--row", '-r', type=int, default=None,
        help="Only run for a specific row")
    subparser.add_argument(
        'name', metavar='NAME', type=str, nargs=1,
        help='the dataset identifier', choices=[d.name for d in datasets])
    subparser.add_argument(
         '--force',  "-f", action='store_true',
         help="redo all the inferences, even if we have already filled out some", )
    subparser.add_argument(
         '--progress',  "-P", action='store_true',
         help="Show a progress bar.", )
    subparser.add_argument(
         '--flush-all',  "-F", action='store_true',
         help="flush the result file after every result.", )
    subparser.set_defaults(func=run_infer)

    subparser = subparsers.add_parser('figure')
    subparser.add_argument(
        'name', metavar='NAME', type=str, nargs=1,
        help='the figure identifier', choices=[f.name for f in figures])
    subparser.set_defaults(func=run_plot)

    args = parser.parse_args()
    log_level = logging.WARNING
    if args.verbosity == 1:
        log_level = logging.INFO
    if args.verbosity >= 2:
        log_level = logging.DEBUG
    logging.basicConfig(
        format='%(asctime)s %(message)s', level=log_level, stream=sys.stdout)

    # Create a new process group and become the leader.
    os.setpgrp()
    k = args.name[0]
    try:
        if k == "all":
            classes = datasets
            if args.func == run_plot:
                classes = figures
            for name, cls in name_map.items():
                if cls in classes:
                    args.func(cls, args)
        else:
            try:
                cls = name_map[k]
            except KeyError as e:
                e.args = (e.args[0] + ". Select from datasets={} or figures={}".format(
                        [d.name for d in datasets], [f.name for f in figures]),)
                raise
            args.func(cls, args)
    except KeyboardInterrupt:
        print("Interrupted! Trying to kill subprocesses")
        os.killpg(0, signal.SIGINT)

if __name__ == "__main__":
    main()
