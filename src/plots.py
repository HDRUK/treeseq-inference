#!/usr/bin/env python3
"""
Code to run simulations, inference methods and generate all plots
in the paper.
"""

import argparse
import collections
import filecmp
import glob
import itertools
import json
import logging
import multiprocessing
import os.path
import random
import shutil
import signal
import statistics
import subprocess
import sys
import tempfile
import time

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as pyplot
import pandas as pd

# import the local copy of msprime in preference to the global one
curr_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(1,os.path.join(curr_dir,'..','msprime'))
import msprime
import msprime_extras
import msprime_fastARG
import msprime_ARGweaver
import msprime_RentPlus
import ARG_metrics

fastARG_executable = os.path.join(curr_dir,'..','fastARG','fastARG')
ARGweaver_executable = os.path.join(curr_dir,'..','argweaver','bin','arg-sample')
smc2arg_executable = os.path.join(curr_dir,'..','argweaver','bin','smc2arg')
RentPlus_executable = os.path.join(curr_dir,'..','RentPlus','RentPlus.jar')
tsinfer_executable = os.path.join(curr_dir,'run_tsinfer.py')
#monkey-patch write.nexus into msprime
msprime.TreeSequence.write_nexus_trees = msprime_extras.write_nexus_trees

#R tree metrics assume tips are numbered from 1 not 0
tree_tip_labels_start_at_0 = False

if sys.version_info[0] < 3:
    raise Exception("Python 3 only")

def cpu_time_colname(tool):
    return tool + "_cputime"

def memory_colname(tool):
    return tool + "_memory"

def n_coalescence_records_colname(tool):
    return tool + "_crecords"

def metric_colnames(metrics_for):
    metric_names = ARG_metrics.get_metric_names()
    return ["{}_{}".format(f, metric) for f in metrics_for for metric in metric_names]

def nanblank(val):
    """hack around a horrible pandas syntax, which puts nan instead of blank strings"""
    return "" if pd.isnull(val) else val

def always_true(*pargs):
    """
    A func that returns True for any input value
    """
    return True

def make_errors(v, p):
    """
    For each sample an error occurs with probability p. Errors are generated by
    sampling values from the stationary distribution, that is, if we have an
    allele frequency of f, a 1 is emitted with probability f and a
    0 with probability 1 - f. Thus, there is a possibility that an 'error'
    will in fact result in the same value.
    """
    w = np.copy(v)
    if p > 0:
        m = v.shape[0]
        frequency = np.sum(v) / m
        # Randomly choose samples with probability p
        samples = np.where(np.random.random(m) < p)[0]
        # Generate observations from the stationary distribution.
        errors = (np.random.random(samples.shape[0]) < frequency).astype(int)
        w[samples] = errors
    return w

def generate_samples(ts, error_p):
    """
    Returns samples with a bits flipped with a specified probability.

    Rejects any variants that result in a fixed column.
    """
    S = np.zeros((ts.sample_size, ts.num_mutations), dtype="u1")
    for variant in ts.variants():
        done = False
        # Reject any columns that have no 1s or no zeros
        while not done:
            S[:,variant.index] = make_errors(variant.genotypes, error_p)
            s = np.sum(S[:, variant.index])
            done = 0 < s < ts.sample_size
    return S

def msprime_name(n, Ne, l, rho, mu, genealogy_seed, mut_seed, directory=None):
    """
    Create a filename for saving an msprime simulation (without extension)
    Other functions add error rates and/or a subsample sizes to the name
    """
    #format mut rate & recomb rate to print non-exponential notation without
    # trailing zeroes 12 dp should be ample for these rates
    rho = "{:.12f}".format(float(rho)).rstrip('0')
    mu = "{:.12f}".format(float(mu)).rstrip('0')
    file = "msprime-n{}_Ne{}_l{}_rho{}_mu{}-gs{}_ms{}".format(int(n), float(Ne), int(l), rho, \
        mu, int(genealogy_seed), int(mut_seed))
    if directory is None:
        return file
    else:
        return os.path.join(directory,file)

def msprime_name_from_row(row, directory=None, error_col=None, subsample_col=None):
    """
    If error_col and subsample_col are None, this is the same as msprime_name()
    but filled out using data from a row. If error_col is a string which exists as
    a column name in the row then add_error_param_to_name() is also called, using
    the error rate specified in that column. Alternatively (e.g. if error_col is a
    number) then it is treated as the error rate to add via add_error_param_to_name().
    The same goes for subsample_col.
    """
    name = msprime_name(row.sample_size, row.Ne, row.length, row.recombination_rate,
        row.mutation_rate, row.seed, row.seed, directory)
    if subsample_col is not None and not pd.isnull(subsample_col):
        if isinstance(subsample_col, str):
            if subsample_col in row:
                name = add_subsample_param_to_name(name, row[subsample_col])
        else:
            name = add_subsample_param_to_name(name, subsample_col)
    if error_col is not None and not pd.isnull(error_col):
        if isinstance(error_col, str):
            if error_col in row:
                name = add_error_param_to_name(name, row[error_col])
        else:
            name = add_error_param_to_name(name, error_col)
    return(name)

def add_subsample_param_to_name(sim_name, subsample_size=None):
    """
    Mark a filename as containing only a subset of the samples of the full sim
    Can be used on msprime output files but also e.g. tsinfer output files
    """
    if subsample_size is not None and not pd.isnull(subsample_size):
        if sim_name.endswith("+") or sim_name.endswith("-"):
            #this is the first param
            return sim_name + "max{}".format(int(subsample_size))
        else:
            return sim_name + "_max{}".format(int(subsample_size))
    else:
        return sim_name
def add_error_param_to_name(sim_name, error_rate=None):
    """
    Append the error param to the msprime simulation filename.
    Only relevant for files downstream of the step where sequence error is added
    """
    if error_rate is not None and not pd.isnull(error_rate):
        if sim_name.endswith("+") or sim_name.endswith("-"):
            #this is the first param
            return sim_name + "_err{}".format(float(error_rate))
        else:
            #this is the first param
            return sim_name + "err{}".format(float(error_rate))
    else:
        return sim_name

def construct_fastarg_name(sim_name, seed, directory=None):
    """
    Returns a fastARG inference filename (without file extension),
    based on a simulation name
    """
    d,f = os.path.split(sim_name)
    return os.path.join(d,'+'.join(['fastarg', f, "fs"+str(int(seed))]))

def fastarg_name_from_msprime_row(row, sim_dir):
    """
    return the fa name based on an msprime sim specified by row
    """
    return construct_fastarg_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample'),
                                  seed=row.seed)


def construct_argweaver_name(sim_name, seed, iteration_number=None):
    """
    Returns an ARGweaver inference filename (without file extension),
    based on a simulation name. The iteration number (used in .smc and .nex output)
    is usually added by the ARGweaver `arg-sample` program,
    in the format .10, .20, etc. (we append an 'i' too, giving
    'i.10', 'i.100', etc
    """
    d,f = os.path.split(sim_name)
    suffix = "ws"+str(int(seed))
    if iteration_number is not None:
        suffix += "_i."+ str(int(iteration_number))
    return os.path.join(d,'+'.join(['aweaver', f, suffix]))

def argweaver_names_from_msprime_row(row, sim_dir):
    """
    return multiple argweaver names based on an msprime sim specified by row
    there is one name per argweaver iteration listed in row.ARGweaver_iterations
    """
    return [construct_argweaver_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample'),
                                     seed=row.seed, iteration_number=it)
                for it in nanblank(row.ARGweaver_iterations).split(",") if it]

def construct_rentplus_name(sim_name):
    """
    Returns an RentPlus inference filename (without file extension),
    based on a simulation name.
    """
    d,f = os.path.split(sim_name)
    return os.path.join(d,'+'.join(['rentpls', f, ""]))

def rentplus_name_from_msprime_row(row, sim_dir):
    """
    return the rentplus name based on an msprime sim specified by row
    """
    return construct_rentplus_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample'))

def construct_tsinfer_name(sim_name, subsample_size=None):
    """
    Returns an MSprime Li & Stevens inference filename.
    If the file is a subset of the original, this can be added to the
    basename in this function, or later using the
    add_subsample_param_to_name() routine.
    """
    d,f = os.path.split(sim_name)
    name = os.path.join(d,'+'.join(['tsinfer', f, ""]))
    if subsample_size is not None and not pd.isnull(subsample_size):
        name = add_subsample_param_to_name(name, subsample_size)
    return name

def tsinfer_name_from_msprime_row(row, sim_dir, subsample_size=None):
    """
    return the tsinfer name based on an msprime sim specified by row
    """
    if subsample_size is None and not pd.isnull(subsample_size):
        return construct_tsinfer_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample'))
    else:
        return construct_tsinfer_name(msprime_name_from_row(row, sim_dir, 'error_rate', 'subsample'),
            subsample_size = subsample_size)


def time_cmd(cmd, stdout=sys.stdout):
    """
    Runs the specified command line (a list suitable for subprocess.call)
    and writes the stdout to the specified file object.
    """
    if sys.platform == 'darwin':
        #on OS X, install gtime using `brew install gnu-time`
        time_cmd = "/usr/local/bin/gtime"
    else:
        time_cmd = "/usr/bin/time"
    full_cmd = [time_cmd, "-f%M %S %U"] + cmd
    with tempfile.TemporaryFile() as stderr:
        exit_status = subprocess.call(full_cmd, stderr=stderr, stdout=stdout)
        stderr.seek(0)
        if exit_status != 0:
            raise ValueError(
                "Error running '{}': status={}:stderr{}".format(
                    " ".join(cmd), exit_status, stderr.read()))

        split = stderr.readlines()[-1].split()
        # From the time man page:
        # M: Maximum resident set size of the process during its lifetime,
        #    in Kilobytes.
        # S: Total number of CPU-seconds used by the system on behalf of
        #    the process (in kernel mode), in seconds.
        # U: Total number of CPU-seconds that the process used directly
        #    (in user mode), in seconds.
        max_memory = int(split[0]) * 1024
        system_time = float(split[1])
        user_time = float(split[2])
    return user_time + system_time, max_memory

def ARGmetric_params_from_row(row):
    """
    Create some ARGmetric params (see ARGmetrics.py) from a row
    We hack the same polytomy seed as inference seed

    This stupidly has to be defined at the top level, since it is
    part of the function passed in to a multiprocessing Pool, and
    hence needs to be 'pickle'able :(
    """
    return {'make_bin_seed':row.seed, 'reps':row.tsinfer_biforce_reps}

class InferenceRunner(object):
    """
    Class responsible for running a single inference tool and returning results for
    the dataframe. Should create results files that are bespoke for each tool, and
    also for each tool, convert these into nexus files that can be used for comparing
    metrics.
    """
    def __init__(self, tool, row, simulations_dir, num_threads, n_rows=None):
        self.tool = tool
        self.row = row
        self.n_rows = n_rows
        self.num_threads = num_threads
        self.base_fn = msprime_name_from_row(row, simulations_dir, 'error_rate',
                'subsample')
        self.source_nexus_file = msprime_name_from_row(row, simulations_dir) + ".nex"
        # This should be set by the run_inference methods.
        self.inferred_nexus_file = None

    def run(self):
        logging.info("Row {}/~{}: running {} inference".format(
            int(self.row[0]),self.n_rows,self.tool))
        logging.debug("parameters = {}".format(self.row.to_dict()))
        if self.tool == "tsinfer":
            ret = self.__run_tsinfer()
        elif self.tool == "fastARG":
            ret = self.__run_fastARG()
        elif self.tool == "ARGweaver":
            ret = self.__run_ARGweaver()
        elif self.tool == "RentPlus":
            ret = self.__run_RentPlus()
        else:
            raise KeyError("unknown tool {}".format(self.tool))
        metrics = ARG_metrics.get_metrics(self.source_nexus_file, self.inferred_nexus_file)
        for metric, value in metrics.items():
            ret[self.tool + "_" + metric] = value
        logging.debug("returning infer results for {} row {} = {}".format(
            self.tool, int(self.row[0]), ret))
        return ret

    def __run_tsinfer(self):

        samples_fn = self.base_fn + ".npy"
        positions_fn = self.base_fn + ".pos.npy"
        time = memory = c_records = poly_sum = poly_ssq = poly_max = n = None
        logging.debug("reading: variant matrix {} & positions {} for msprime inference".format(
            samples_fn, positions_fn))
        scaled_recombination_rate = 4 * self.row.recombination_rate * self.row.Ne
        inferred_ts, time, memory = self.run_tsinfer(
            samples_fn, positions_fn, self.row.length, scaled_recombination_rate,
            self.row.error_rate, num_threads=self.num_threads)
        if 'tsinfer_subset' in self.row:
            logging.debug("writing trees for only a subset of {} / {} tips".format(
                int(self.row.tsinfer_subset), inferred_ts.sample_size))
            inferred_ts = inferred_ts.simplify(list(range(int(self.row.tsinfer_subset))))
            out_fn = construct_tsinfer_name(self.base_fn, int(self.row.tsinfer_subset))
        else:
            out_fn = construct_tsinfer_name(self.base_fn)
        inferred_ts.dump(out_fn + ".hdf5")
        self.inferred_nexus_file = out_fn + ".nex"
        with open(self.inferred_nexus_file, "w+") as out:
            #tree metrics assume tips are numbered from 1 not 0
            inferred_ts.write_nexus_trees(out, tree_labels_between_variants=True,
                zero_based_tip_numbers=tree_tip_labels_start_at_0)
        poly_sum = poly_ssq = poly_max = 0
        for e in inferred_ts.edgesets():
            poly_sum += len(e.children)
            poly_ssq += len(e.children)**2
            poly_max = max(len(e.children), poly_max)
        poly_mean = poly_sum / inferred_ts.num_edgesets
        n = inferred_ts.num_edgesets
        return  {
            cpu_time_colname(self.tool): time,
            memory_colname(self.tool): memory,
            n_coalescence_records_colname(self.tool): c_records,
            'tsinfer_mean_polytomy': poly_mean if n else None,
            'tsinfer_var_polytomy': ((poly_ssq - poly_sum**2/n)/ (n-1)) if n and n>1 else None,
            'tsinfer_max_polytomy': poly_max
        }

    def __run_fastARG(self):
        inference_seed = self.row.seed  # TODO do we need to specify this separately?
        infile = self.base_fn + ".hap"
        time = None
        memory = None
        c_records = None
        logging.debug("reading: {} for fastARG inference".format(infile))
        out_fn = construct_fastarg_name(self.base_fn, inference_seed) + ".nex"
        inferred_ts, time, memory = self.run_fastarg(infile, self.row.length, inference_seed)
        with open(out_fn , "w+") as out:
            #the treeseq output by run_fastarg() is already averaged between regions
            inferred_ts.write_nexus_trees(out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
        c_records = inferred_ts.num_edgesets
        return {
            cpu_time_colname(self.tool): time,
            memory_colname(self.tool): memory,
            n_coalescence_records_colname(self.tool): c_records
        }

    def __run_RentPlus(self):
        infile = self.base_fn + ".dat"
        time = None
        memory = None
        logging.debug("reading: {} for RentPlus inference".format(infile))
        treefile, num_tips, time, memory = self.run_rentplus(infile, self.row.length)
        if treefile:
            self.inferred_nexus_file = construct_rentplus_name(self.base_fn) + ".nex"
            with open(self.inferred_nexus_file, "w+") as out:
                msprime_RentPlus.RentPlus_trees_to_nexus(treefile, out, self.row.length,
                    num_tips, zero_based_tip_numbers=tree_tip_labels_start_at_0)
        return {
            cpu_time_colname(self.tool): time,
            memory_colname(self.tool): memory,
            n_coalescence_records_colname(self.tool): None
        }


    def __run_ARGweaver(self):
        inference_seed = self.row.seed  # TODO do we need to specify this separately?
        infile = self.base_fn + ".sites"
        time = None
        memory = None
        c_records = []
        iteration_ids = []
        stats_file = None
        logging.debug("reading: {} for ARGweaver inference".format(infile))
        out_fn = construct_argweaver_name(self.base_fn, inference_seed)
        iteration_ids, stats_file, time, memory = self.run_argweaver(
            infile, self.row.Ne, self.row.recombination_rate, self.row.mutation_rate,
            out_fn, inference_seed, int(self.row.aw_n_out_samples),
            self.row.aw_iter_out_freq, int(self.row.aw_burnin_iters),
            verbose = logging.getLogger().isEnabledFor(logging.DEBUG))
        #now must convert all of the .smc files to .nex format
        for it in iteration_ids:
            base = construct_argweaver_name(self.base_fn, inference_seed, it)
            with open(base + ".nex", "w+") as out:
                msprime_ARGweaver.ARGweaver_smc_to_nexus(
                    base+".smc.gz", out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
            try:
                with open(base+".msp", "w+") as msprime_txtrecs:
                    msprime_ARGweaver.ARGweaver_smc_to_msprime_txts(
                        smc2arg_executable, base, msprime_txtrecs,
                        override_assertions=True)
                    inferred_ts = msprime.load_txt(msprime_txtrecs.name).simplify()
                    c_records.append(inferred_ts.num_edgesets)
            except AssertionError:
                logging.warning("smc2arg bug encountered converting '{}' to TS. Ignoring this row".format(
                    base+".msp"))
                #smc2arg cyclical bug: ignore this conversion
                pass
        results = {
            cpu_time_colname(self.tool): time,
            memory_colname(self.tool): memory,
            n_coalescence_records_colname(self.tool): statistics.mean(c_records) if len(c_records) else None,
            "ARGweaver_iterations": ",".join(iteration_ids),
            "ARGweaver_stats_file":stats_file,
        }
        return results

    @staticmethod
    def run_tsinfer(sample_fn, positions_fn, length, rho, error_probability, num_threads=1):
        with tempfile.NamedTemporaryFile("w+") as ts_out:
            cmd = [
                sys.executable, tsinfer_executable, sample_fn, positions_fn,
                "--length", str(int(length)), "--recombination-rate", str(rho),
                "--error-probability", str(error_probability),
                "--threads", str(num_threads), ts_out.name]
            cpu_time, memory_use = time_cmd(cmd)
            ts_simplified = msprime.load(ts_out.name)
        return ts_simplified, cpu_time, memory_use

    @staticmethod
    def run_fastarg(file_name, seq_length, seed):
        with tempfile.NamedTemporaryFile("w+") as fa_out, \
                tempfile.NamedTemporaryFile("w+") as tree, \
                tempfile.NamedTemporaryFile("w+") as muts, \
                tempfile.NamedTemporaryFile("w+") as fa_revised:
            cmd = msprime_fastARG.get_cmd(fastARG_executable, file_name, seed)
            cpu_time, memory_use = time_cmd(cmd, fa_out)
            logging.debug("ran fastarg for seq length {} [{} s]: '{}'".format(seq_length, cpu_time, cmd))
            var_pos = msprime_fastARG.variant_positions_from_fastARGin_name(file_name)
            root_seq = msprime_fastARG.fastARG_root_seq(fa_out)
            msprime_fastARG.fastARG_out_to_msprime_txts(
                    fa_out, var_pos, tree, muts, seq_len=seq_length)
            inferred_ts = msprime_fastARG.msprime_txts_to_fastARG_in_revised(
                    tree, muts, root_seq, fa_revised, simplify=True)
            try:
                assert filecmp.cmp(file_name, fa_revised.name, shallow=False), \
                    "{} and {} differ".format(file_name, fa_revised.name)
            except Exception as e:
                debug_file = os.path.join(os.path.dirname(fa_revised.name), "bad.hap")
                shutil.copyfile(fa_revised.name, debug_file)
                e.args = (e.args[0] + ". File '{}' copied to '{}' for debugging".format(
                    fa_revised.name, debug_file),)
                raise
            return inferred_ts, cpu_time, memory_use

    @staticmethod
    def run_rentplus(file_name, seq_length):
        """
        runs RentPlus, returning the output filename, the total CPU, & max mem
        must check here if we are using 0..1 positions (infinite sites) or integers
        """
        haplotype_lines = 0
        integer_positions = True
        with open(file_name, "r+") as infile:
            for pos in next(infile).split():
                try:
                    dummy = int(pos)
                except ValueError:
                    integer_positions = False
            for line in infile:
                if line.rstrip():
                    haplotype_lines += 1
        cmd = ["java", "-jar", RentPlus_executable]
        cmd += [file_name] if integer_positions else ['-l', seq_length, file_name]
        with tempfile.NamedTemporaryFile("w+") as script_output:
            cpu_time, memory_use = time_cmd(cmd, script_output)
        logging.debug("ran RentPlus for {} haplotypes with seq length {} [{} s]: '{}'".format(
            haplotype_lines, seq_length, cpu_time, cmd))
        #we cannot back-convert RentPlus output to treeseq form - just return the trees file
        assert os.path.isfile(file_name + ".trees"), 'No trees file created when running Rent+'
        #we might also want to look at TMRCAs (file_name + '.Tmrcas')
        return file_name + ".trees", haplotype_lines, cpu_time, memory_use

    @staticmethod
    def run_argweaver(
            sites_file, Ne, recombination_rate, mutation_rate, path_prefix, seed,
            MSMC_samples, sample_step, burnin_iterations=0, verbose=False):
        """
        this produces a whole load of .smc files labelled <path_prefix>i.0.smc,
        <path_prefix>i.10.smc, etc.

        Returns the iteration numbers ('0', '10', '20' etc), the name of the
        statistics file, the total CPU time, and the max memory usage.
        """
        cpu_time = []
        memory_use = []
        burn_prefix = None
        try:
            exe = [ARGweaver_executable, '--sites', sites_file.name if hasattr(sites_file, "name") else sites_file,
                   '--popsize', str(Ne),
                   '--recombrate', str(recombination_rate),
                   '--mutrate', str(mutation_rate),
                   '--overwrite']
            if not verbose:
                exe += ['--quiet']
            if seed is not None:
                exe += ['--randseed', str(int(seed))]
            if burnin_iterations > 0:
                burn_in = str(int(burnin_iterations))
                burn_prefix = path_prefix+"_burn"
                logging.info("== Burning in ARGweaver MCMC using {} steps ==".format(burn_in))
                c, m = time_cmd(exe + ['--iters', burn_in,
                                       '--sample-step', burn_in,
                                       '--output', burn_prefix])
                cpu_time.append(c)
                memory_use.append(m)
                #if burn_in, read from the burn in arg file, rather than the original .sites
                exe += ['--arg', burn_prefix+"."+ burn_in +".smc.gz"]
            else:
                exe += ['--sites', sites_file]

            new_prefix = path_prefix + "_i" #we append a '_i' to mark iteration number
            iterations = int(sample_step * (MSMC_samples-1))
            exe += ['--output', new_prefix]
            exe += ['--iters', str(iterations)]
            exe += ['--sample-step', str(int(sample_step))]
            logging.info("== Running ARGweaver for {} steps to collect {} samples ==".format( \
                int(iterations), MSMC_samples))
            logging.debug("== ARGweaver command is {} ==".format(" ".join(exe)))
            c, m = time_cmd(exe)
            cpu_time.append(c)
            memory_use.append(m)

            smc_prefix = new_prefix + "." #the arg-sample program adds .iteration_num
            saved_iterations = [f[len(smc_prefix):-7] for f in glob.glob(smc_prefix + "*" + ".smc.gz")]
            new_stats_file_name = path_prefix+".stats"

            #concatenate all the stats together
            with open(new_stats_file_name, "w+") as stats:
                if burn_prefix:
                    shutil.copyfileobj(open(burn_prefix + ".stats"), stats)
                    print("\n", file=stats)
                shutil.copyfileobj(open(new_prefix + ".stats"), stats)
            #cannot translate these to msprime ts objects, as smc2arg does not work
            #see https://github.com/mdrasmus/argweaver/issues/20
            return saved_iterations, new_stats_file_name, sum(cpu_time), max(memory_use)
        except ValueError as e:
            if 'src/argweaver/sample_thread.cpp:517:' in str(e):
                logging.info("Hit argweaver bug https://github.com/mcveanlab/treeseq-inference/issues/25. Skipping")
                return "Simulation error", "NA", None, None
            else:
                raise


def infer_worker(work):
    """
    Entry point for running a single inference task in a worker process.
    """
    tool, row, simulations_dir, num_threads, n_rows = work
    runner = InferenceRunner(tool, row, simulations_dir, num_threads, n_rows)
    result = runner.run()
    return int(row[0]), result


class Dataset(object):
    """
    A dataset is some collection of simulations and associated data.
    """
    name = None
    """
    Each dataset has a unique name. This is used as the prefix for the data
    file and raw_data_dir directory. Within this, replicate instances of datasets
    (each with a different RNG seed) are saved under the seed number
    """

    data_dir = "data"

    tools = [
        # Disabling ARGweaver initially as it's too slow.
        # "ARGweaver":
        # Disabling FastARG until the input mechanism is fixed.
        # "fastARG"
        "RentPlus",
        "tsinfer"]

    def __init__(self):
        self.data_path = os.path.abspath(
            os.path.join(self.data_dir, "{}".format(self.name)))
        self.data_file = self.data_path + ".csv"
        self.param_file = self.data_path + ".json"
        self.raw_data_dir = os.path.join(self.data_dir, "raw__NOBACKUP__", self.name)
        self.simulations_dir = os.path.join(self.raw_data_dir, "simulations")
        self.last_data_write_time = time.time()

    def load_data(self):
        self.data = pd.read_csv(self.data_file)

    def dump_data(self, write_index=False, force_flush=True):
        """
        Dumps data if it hasn't been written in the last 30 seconds. If force is true,
        write it out anyway.
        """
        now = time.time()
        if force_flush or (now - self.last_data_write_time) > 30:
            logging.info("Flushing data file")
            self.data.to_csv(self.data_file, index=write_index)
            self.last_data_write_time = time.time()

    #
    # Main entry points.
    #
    def setup(self, args):
        """
        Creates the dataframe and storage directories and then runs the initial
        simulations.
        """
        if os.path.exists(self.simulations_dir):
            shutil.rmtree(self.simulations_dir)
            logging.info("Deleting dir {}".format(self.simulations_dir))
        os.makedirs(self.simulations_dir)
        self.verbosity = args.verbosity
        logging.info("Creating dir {}".format(self.simulations_dir))
        self.data = self.run_simulations(args.replicates, args.seed)
        # Other result columns are added later during the infer step.
        self.dump_data(write_index=True)

    def infer(
            self, num_processes, num_threads, force=False, specific_tool=None,
            specific_row=None):
        """
        Runs the main inference processes and stores results in the dataframe.
        can 'force' all rows to be (re)run, or specify a specific row to run.
        """
        self.load_data()
        tools = self.tools
        if specific_tool is not None:
            if specific_tool not in self.tools:
                raise ValueError("Tool '{}' not recognised: options = {}".format(
                    specific_tool, list(self.tools)))
            tools = [specific_tool]
        row_ids = self.data.index
        if specific_row is not None:
            if specific_row < 0 or specific_row > len(self.data.index):
                raise ValueError("Row {} out of bounds".format(specific_row))
            row_ids = [specific_row]
        work = []
        for row_id in row_ids:
            row = self.data.iloc[row_id]
            for tool in tools:
                # All values that are unset should be NaN, so we only run those that
                # haven't been filled in already. This allows us to stop and start the
                # infer processes without having to start from scratch each time.
                if force or pd.isnull(row[cpu_time_colname(tool)]):
                    work.append(
                        (tool, row, self.simulations_dir, num_threads, len(self.data.index)))
                else:
                    logging.info(
                        "Data row {} is filled out for {} inference: skipping".format(
                            row_id, tool))
        logging.info(
            "running {} inference trials (max {} tools over {} of {} rows) with {} "
            "processes and {} threads".format(
                len(work), len(self.tools), int(np.ceil(len(work)/len(self.tools))),
                len(self.data.index), num_processes, num_threads))

        # Randomise the order that work is done in so that we get results for all parts
        # of the plots through rather than
        random.shuffle(work)
        if num_processes > 1:
            with multiprocessing.Pool(processes=num_processes) as pool:
                for row_id, updated in pool.imap_unordered(infer_worker, work):
                    for k, v in updated.items():
                        self.data.ix[row_id, k] = v
                    self.dump_data(force_flush=False)
        else:
            # When we have only one process it's easier to keep everything in the same
            # process for debugging.
            for row_id, updated in map(infer_worker, work):
                for k, v in updated.items():
                    self.data.ix[row_id, k] = v
                self.dump_data(force_flush=False)
        self.dump_data(force_flush=True)

    #
    # Utilities for running simulations and saving files.
    #
    def single_simulation(self, n, Ne, l, rho, mu, seed, mut_seed=None,
        discretise_mutations=True):
        """
        The standard way to run one msprime simulation for a set of parameter
        values. Saves the output to an .hdf5 file, and also saves variant files
        for use in fastARG (a .hap file, in the format specified by
        https://github.com/lh3/fastARG#input-format) ARGweaver (a .sites file:
        http://mdrasmus.github.io/argweaver/doc/#sec-file-sites) tsinfer
        (currently a numpy array containing the variant matrix)

        mutation_seed is not yet implemented, but should allow the same
        ancestry to be simulated (if the same genealogy_seed is given) but have
        different mutations thrown onto the trees (even with different
        mutation_rates)

        Returns a tuple of treesequence, filename (without file type extension)
        """
        logging.info(
            "Running simulation for n = {}, l = {}, rho={}, mu = {} seed={}".format(
                n, l, rho, mu, seed))
        # Since we want to have a finite site model, we force the recombination map
        # to have exactly l loci with a recombination rate of rho between them.
        recombination_map = msprime.RecombinationMap.uniform_map(l, rho, l)
        # We need to rejection sample any instances that we can't discretise under
        # the current model. The simplest way to do this is to have a local RNG
        # which we seed with the specified seed.
        rng = random.Random(seed)
        # TODO replace this with a proper finite site mutation model in msprime.
        done = False
        while not done:
            sim_seed = rng.randint(1, 2**31)
            ts = msprime.simulate(
                n, Ne, recombination_map=recombination_map, mutation_rate=mu,
                random_seed=sim_seed)
            if discretise_mutations:
                try:
                    ts = msprime_extras.discretise_mutations(ts)
                    done = True
                except ValueError as ve:
                    logging.info("Rejecting simulation: seed={}: {}".format(sim_seed, ve))
            else:
                done=True
        # Here we might want to iterate over mutation rates for the same
        # genealogy, setting a different mut_seed so that we can see for
        # ourselves the effect of mutation rate variation on a single topology
        # but for the moment, we don't bother, and simply write
        # mut_seed==genealogy_seed
        sim_fn = msprime_name(n, Ne, l, rho, mu, seed, seed, self.simulations_dir)
        logging.debug("writing {}.hdf5".format(sim_fn))
        ts.dump(sim_fn+".hdf5", zlib_compression=True)
        return ts, sim_fn

    @staticmethod
    def save_variant_matrices(ts, fname, error_rate=None, infinite_sites=True):
        if ts.num_mutations>0:
            S = generate_samples(ts, error_rate or 0)
            filename = add_error_param_to_name(fname, error_rate)
            outfile = filename + ".npy"
            logging.debug("writing variant matrix to {} for tsinfer".format(outfile))
            np.save(outfile, S)
            outfile = filename + ".pos.npy"
            pos = np.array([v.position for v in ts.variants()])
            logging.debug("writing variant positions to {} for tsinfer".format(outfile))
            np.save(outfile, pos)
            if infinite_sites: #for infinite sites, assume we have discretised mutations to ints
                assert all(p.is_integer() for p in pos), \
                    "Variant positions are not all integers in {}".format()
            logging.debug("writing variant matrix to {}.hap for fastARG".format(filename))
            with open(filename+".hap", "w+") as fastarg_in:
                msprime_fastARG.variant_matrix_to_fastARG_in(S.T, pos, fastarg_in)
            logging.debug("writing variant matrix to {}.sites for ARGweaver".format(filename))
            with open(filename+".sites", "w+") as argweaver_in:
                msprime_ARGweaver.variant_matrix_to_ARGweaver_in(S.T, pos,
                        ts.get_sequence_length(), argweaver_in, infinite_sites=infinite_sites)
            logging.debug("writing variant matrix to {}.dat for RentPlus".format(filename))
            with open(filename+".dat", "wb+") as rentplus_in:
                msprime_RentPlus.variant_matrix_to_RentPlus_in(S.T, pos,
                        ts.get_sequence_length(), rentplus_in, infinite_sites=infinite_sites)
        else:
            #No variants. We should be able to get away with not creating any files
            #and the infer step will simply skip this simulation
            logging.info("No variants in this sample, so no files created for this simulation")


class MetricsByMutationRateDataset(Dataset):
    """
    Dataset for Figure 1
    Accuracy of ARG inference (measured by various statistics)
    tending to fully accurate as mutation rate increases
    """
    name = "metrics_by_mutation_rate"

    default_replicates = 10
    default_seed = 123

    def run_simulations(self, replicates, seed):
        if replicates is None:
            replicates = self.default_replicates
        if seed is None:
            seed = self.default_seed
        rng = random.Random(seed)
        cols = [
            "sample_size", "Ne", "length", "recombination_rate", "mutation_rate",
            "error_rate", "seed"]
        # Variable parameters
        mutation_rates = np.logspace(-8, -5, num=6)[:-1] * 1.5
        error_rates = [0, 0.01, 0.1]
        sample_sizes = [10, 50]

        # Fixed parameters
        Ne = 5000
        length = 10000
        recombination_rate = 2.5e-8
        num_rows = replicates * len(mutation_rates) * len(error_rates) * len(sample_sizes)
        data = pd.DataFrame(index=np.arange(0, num_rows), columns=cols)
        row_id = 0
        for replicate in range(replicates):
            for mutation_rate in mutation_rates:
                for sample_size in sample_sizes:
                    done = False
                    while not done:
                        replicate_seed = rng.randint(1, 2**31)
                        # Run the simulation
                        ts, fn = self.single_simulation(
                            sample_size, Ne, length, recombination_rate, mutation_rate,
                            replicate_seed, replicate_seed,
                            #discretise_mutations=True)
                            discretise_mutations=False) #stop doing Jerome's discretising step!
                        # Reject this instances if we got no mutations.
                        done = ts.get_num_mutations() > 0
                    with open(fn +".nex", "w+") as out:
                        ts.write_nexus_trees(
                            out, zero_based_tip_numbers=tree_tip_labels_start_at_0)
                    # Add the rows for each of the error rates in this replicate
                    for error_rate in error_rates:
                        row = data.iloc[row_id]
                        row_id += 1
                        row.sample_size = sample_size
                        row.recombination_rate = recombination_rate
                        row.mutation_rate = mutation_rate
                        row.length = length
                        row.Ne = Ne
                        row.seed = replicate_seed
                        row.error_rate = error_rate
                        self.save_variant_matrices(ts, fn, error_rate,
                            #infinite_sites=True)
                            infinite_sites=False)
        return data


######################################
#
# Figures
#
######################################

class Figure(object):
    """
    Superclass of all figures. Each figure depends on a dataset.
    """
    datasetClass = None
    name = None
    figures_dir = "figures"
    """
    Each figure has a unique name. This is used as the identifier and the
    file name for the output plots.
    """

    def __init__(self):
        self.dataset = self.datasetClass()
        self.dataset.load_data()

    def savefig(self, figure):
        filename = os.path.join(self.figures_dir, "{}.pdf".format(self.name))
        figure.savefig(filename)

    def plot(self):
        raise NotImplementedError()


class AllMetricsByMutationRateFigure(Figure):
    """
    Simple figure that shows all the metrics at the same time.
    """
    datasetClass = MetricsByMutationRateDataset
    name = "all_metrics_vs_mutrate"

    def plot(self):
        df = self.dataset.data
        error_rates = df.error_rate.unique()
        sample_sizes = df.sample_size.unique()

        tools = collections.OrderedDict([
            ("tsinfer", "blue"),
            ("RentPlus", "red"),
        ])
        metrics = ARG_metrics.get_metric_names()
        fig, axes = pyplot.subplots(len(metrics), 3, figsize=(12, 30))
        lines = []
        for j, metric in enumerate(metrics):
            for k, error_rate in enumerate(error_rates):
                ax = axes[j][k]
                if j == 0:
                    ax.set_title("Error = {}".format(error_rate))
                if k == 0:
                    ax.set_ylabel(metric + " metric")
                if j == len(metrics) - 1:
                    ax.set_xlabel("Mutation rate")
                for n, linestyle in zip(sample_sizes, ["-", "-."]):
                    df_s = df[np.logical_and(df.sample_size == n, df.error_rate == error_rate)]
                    group = df_s.groupby(["mutation_rate"])
                    group_mean = group.mean()
                    for tool in tools.keys():
                        ax.semilogx(
                            group_mean[tool + "_" + metric], linestyle, color=tools[tool])
                        # ax.plot(group_mean[tool + "_" + metric])

        self.savefig(fig)


class MetricByMutationRateFigure(Figure):
    """
    Superclass of the metric by mutations rate figure. Each subclass should be a
    single figure for a particular metric.
    """
    datasetClass = MetricsByMutationRateDataset


    def plot(self):
        df = self.dataset.data
        error_rates = df.error_rate.unique()
        sample_sizes = df.sample_size.unique()

        # TODO move this into the superclass so that we have consistent styling.
        tool_colours = collections.OrderedDict([
            ("tsinfer", "blue"),
            ("RentPlus", "red"),
        ])
        tool_markers = collections.OrderedDict([
            ("tsinfer", "o"),
            ("RentPlus", "s"),
        ])
        tools = list(tool_colours.keys())
        linestyles = ["-", ":"]
        fig, axes = pyplot.subplots(1, 3, figsize=(12, 6), sharey=True)
        lines = []
        for k, error_rate in enumerate(error_rates):
            ax = axes[k]
            ax.set_title("Error = {}".format(error_rate))
            ax.set_xlabel("Mutation rate")
            if k == 0:
                ax.set_ylabel(self.metric + " metric")
            for n, linestyle in zip(sample_sizes, linestyles):
                df_s = df[np.logical_and(df.sample_size == n, df.error_rate == error_rate)]
                group = df_s.groupby(["mutation_rate"])
                group_mean = group.mean()
                for tool in tools:
                    ax.semilogx(
                        group_mean[tool + "_" + self.metric], linestyle,
                        color=tool_colours[tool],
                        marker=tool_markers[tool])

        axes[0].set_ylim(self.ylim)

        # Create legends from custom artists
        artists = [
            pyplot.Line2D((0,1),(0,0), color=tool_colours[tool],
                marker=tool_markers[tool], linestyle='')
            for tool in tools]
        first_legend = axes[0].legend(
            artists, tools, numpoints=3, loc="upper center")
            # bbox_to_anchor=(0.0, 0.1))
        # ax = pyplot.gca().add_artist(first_legend)
        artists = [
            pyplot.Line2D(
                (0,0),(0,0), color="black", linestyle=linestyle, linewidth=2)
            for linestyle in linestyles]
        axes[-1].legend(
            artists, ["Sample size = {}".format(n) for n in sample_sizes],
            loc="upper center")
        self.savefig(fig)


class RFRootedMetricByMutationsRateFigure(MetricByMutationRateFigure):
    name = "rf_rooted_by_mutation_rate"
    metric = "RFrooted"
    ylim = None


class KCRootedMetricByMutationsRateFigure(MetricByMutationRateFigure):
    name = "kc_rooted_by_mutation_rate"
    metric = "KCrooted"
    ylim = (0, 110)


def run_setup(cls, args):
    f = cls()
    f.setup(args)

def run_infer(cls, args):
    logging.info("Inferring {}".format(cls.name))
    f = cls()
    f.infer(args.processes, args.threads, args.force, args.tool, args.row)

def run_plot(cls, args):
    f = cls()
    f.plot()


def main():
    datasets = Dataset.__subclasses__()
    figures = [
        AllMetricsByMutationRateFigure,
        RFRootedMetricByMutationsRateFigure,
        KCRootedMetricByMutationsRateFigure,
    ]
    name_map = dict([(d.name, d) for d in datasets + figures])
    parser = argparse.ArgumentParser(
        description="Set up base data, generate inferred datasets, process datasets and plot figures.")
    parser.add_argument('--verbosity', '-v', action='count', default=0)
    subparsers = parser.add_subparsers()
    subparsers.required = True
    subparsers.dest = 'command'

    subparser = subparsers.add_parser('setup')
    subparser.add_argument(
        'name', metavar='NAME', type=str, nargs=1,
        help='the dataset identifier', choices=[d.name for d in datasets])
    subparser.add_argument(
         '--replicates', '-r', type=int, help="number of replicates")
    subparser.add_argument(
         '--seed', '-s', type=int, help="use a non-default RNG seed")
    subparser.add_argument(
         '--hack_finite_sites', action='store_true',
         help="Mutations at the same (integer) location are superimposed, not shifted along")
    subparser.set_defaults(func=run_setup)

    subparser = subparsers.add_parser('infer')
    subparser.add_argument(
        "--processes", '-p', type=int, default=1,
        help="number of worker processes")
    subparser.add_argument(
        "--threads", '-t', type=int, default=1,
        help="number of threads per worker process (for supporting tools)")
    subparser.add_argument(
        "--tool", '-T', default=None,
        help="Only run this specific tool")
    subparser.add_argument(
        "--row", '-r', type=int, default=None,
        help="Only run for a specific row")
    subparser.add_argument(
        'name', metavar='NAME', type=str, nargs=1,
        help='the dataset identifier', choices=[d.name for d in datasets])
    subparser.add_argument(
         '--force',  "-f", action='store_true',
         help="redo all the inferences, even if we have already filled out some", )
    subparser.set_defaults(func=run_infer)

    subparser = subparsers.add_parser('figure')
    subparser.add_argument(
        'name', metavar='NAME', type=str, nargs=1,
        help='the figure identifier', choices=[f.name for f in figures])
    subparser.set_defaults(func=run_plot)

    args = parser.parse_args()
    log_level = logging.WARNING
    if args.verbosity == 1:
        log_level = logging.INFO
    if args.verbosity >= 2:
        log_level = logging.DEBUG
    logging.basicConfig(
        format='%(asctime)s %(message)s', level=log_level, stream=sys.stdout)

    # Create a new process group and become the leader.
    os.setpgrp()
    k = args.name[0]
    try:
        if k == "all":
            classes = datasets
            if args.func == run_plot:
                classes = figures
            for name, cls in name_map.items():
                if cls in classes:
                    args.func(cls, args)
        else:
            try:
                cls = name_map[k]
                args.func(cls, args)
            except KeyError as e:
                e.args = (e.args[0] + ". Select from datasets={} or figures={}".format(
                        [d.name for d in datasets], [f.name for f in figures]),)
                raise
    except KeyboardInterrupt:
        print("Interrupted! Trying to kill subprocesses")
        os.killpg(0, signal.SIGINT)

if __name__ == "__main__":
    main()
